"""
EXAONE-3.5-32B Server with MCP Host functionality
LLMì´ MCP ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  í˜¸ì¶œí•˜ëŠ” ê¸°ëŠ¥ í¬í•¨
"""
import logging
import sys
import time
from datetime import datetime

from transformers import AutoModelForCausalLM, AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
import torch
import httpx
import json
import asyncio
import os
import re
import math
from typing import Optional

# ========== ë¡œê¹… ì„¤ì • ==========
LOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)-12s | %(message)s"
LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

logging.basicConfig(
    level=logging.DEBUG,
    format=LOG_FORMAT,
    datefmt=LOG_DATE_FORMAT,
    handlers=[logging.StreamHandler(sys.stdout)]
)

# ë¡œê±° ìƒì„±
logger = logging.getLogger("LLM_MCP")
logger.setLevel(logging.DEBUG)

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì ˆ
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)

logger.info("=" * 70)
logger.info(f"LLM MCP ì„œë²„ ì‹œì‘: {datetime.now().isoformat()}")
logger.info("=" * 70)

app = FastAPI(title="EXAONE-3.5-32B Server + MCP Host")


def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km) - Haversine ê³µì‹"""
    R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)

    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)

    a = math.sin(delta_lat / 2) ** 2 + \
        math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2) ** 2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    return R * c


def calculate_nearby_places(places: list) -> list:
    """ê° ì¥ì†Œì— ëŒ€í•´ ê°€ê¹Œìš´ ì¥ì†Œ ì •ë³´ ì¶”ê°€"""
    for i, place in enumerate(places):
        try:
            lat1 = float(place.get("mapy", 0))
            lon1 = float(place.get("mapx", 0))
            if lat1 == 0 or lon1 == 0:
                continue

            nearby = []
            for j, other in enumerate(places):
                if i == j:
                    continue
                try:
                    lat2 = float(other.get("mapy", 0))
                    lon2 = float(other.get("mapx", 0))
                    if lat2 == 0 or lon2 == 0:
                        continue

                    dist = haversine_distance(lat1, lon1, lat2, lon2)
                    if dist < 5:  # 5km ì´ë‚´
                        nearby.append(f"{other.get('title')}({dist:.1f}km)")
                except:
                    continue

            place["nearby"] = nearby[:3]  # ê°€ê¹Œìš´ ì¥ì†Œ ìµœëŒ€ 3ê°œ
        except:
            continue

    return places


def filter_by_geographic_cluster(places: list, max_radius_km: float = 10.0) -> list:
    """
    ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì§„ ì¥ì†Œ í•„í„°ë§

    ì „ëµ:
    1. ì¢Œí‘œê°€ ìˆëŠ” ì¥ì†Œë“¤ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
    2. ì¤‘ì‹¬ì ì—ì„œ max_radius_km ì´ë‚´ì˜ ì¥ì†Œë§Œ ì„ íƒ
    3. ì¢Œí‘œ ì—†ëŠ” ì¥ì†ŒëŠ” ìœ ì§€ (ì œì™¸í•˜ë©´ ê²°ê³¼ê°€ ë„ˆë¬´ ì ì„ ìˆ˜ ìˆìŒ)
    """
    # ì¢Œí‘œê°€ ìˆëŠ” ì¥ì†Œë“¤ ë¶„ë¦¬
    places_with_coords = []
    places_without_coords = []

    for place in places:
        try:
            lat = float(place.get("mapy", 0))
            lon = float(place.get("mapx", 0))
            if lat != 0 and lon != 0:
                places_with_coords.append((place, lat, lon))
            else:
                places_without_coords.append(place)
        except:
            places_without_coords.append(place)

    if len(places_with_coords) < 2:
        print(f"[CLUSTER] Not enough coords ({len(places_with_coords)}), returning all {len(places)} places")
        return places

    # ì¤‘ì‹¬ì  ê³„ì‚° (ë‹¨ìˆœ í‰ê· )
    avg_lat = sum(p[1] for p in places_with_coords) / len(places_with_coords)
    avg_lon = sum(p[2] for p in places_with_coords) / len(places_with_coords)
    print(f"[CLUSTER] Center point: ({avg_lat:.6f}, {avg_lon:.6f})")

    # ì¤‘ì‹¬ì ì—ì„œ ê°€ê¹Œìš´ ì¥ì†Œë§Œ ì„ íƒ
    filtered_with_coords = []
    excluded = []
    for place, lat, lon in places_with_coords:
        dist = haversine_distance(avg_lat, avg_lon, lat, lon)
        if dist <= max_radius_km:
            filtered_with_coords.append(place)
        else:
            excluded.append(f"{place.get('title', 'Unknown')}({dist:.1f}km)")

    if excluded:
        print(f"[CLUSTER] Excluded {len(excluded)} places beyond {max_radius_km}km: {excluded[:5]}")

    # ê²°ê³¼ í•©ì¹˜ê¸° (ì¢Œí‘œ ìˆëŠ” ê²ƒ + ì¢Œí‘œ ì—†ëŠ” ê²ƒ)
    result = filtered_with_coords + places_without_coords
    print(f"[CLUSTER] Filtered: {len(places)} â†’ {len(result)} places (radius={max_radius_km}km)")

    return result


def optimize_route_order(places: list) -> list:
    """
    Greedy ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë™ì„  ìµœì í™” (ê°€ê¹Œìš´ ìˆœì„œë¡œ ì •ë ¬)

    ì‹œì‘ì : ì²« ë²ˆì§¸ ì¥ì†Œ
    ë‹¤ìŒ ì¥ì†Œ: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë¯¸ë°©ë¬¸ ì¥ì†Œ
    """
    if len(places) < 2:
        return places

    # ì¢Œí‘œê°€ ìˆëŠ” ì¥ì†Œë§Œ ìµœì í™” ëŒ€ìƒ
    coords = []
    for i, place in enumerate(places):
        try:
            lat = float(place.get("mapy", 0))
            lon = float(place.get("mapx", 0))
            if lat != 0 and lon != 0:
                coords.append((i, lat, lon))
        except:
            pass

    if len(coords) < 2:
        return places

    # Greedy TSP
    visited = set()
    route = [coords[0][0]]  # ì²« ë²ˆì§¸ ì¥ì†Œë¶€í„° ì‹œì‘
    visited.add(coords[0][0])
    current_lat, current_lon = coords[0][1], coords[0][2]

    while len(visited) < len(coords):
        nearest = None
        nearest_dist = float('inf')

        for idx, lat, lon in coords:
            if idx not in visited:
                dist = haversine_distance(current_lat, current_lon, lat, lon)
                if dist < nearest_dist:
                    nearest_dist = dist
                    nearest = (idx, lat, lon)

        if nearest:
            route.append(nearest[0])
            visited.add(nearest[0])
            current_lat, current_lon = nearest[1], nearest[2]

    # ì¢Œí‘œ ì—†ëŠ” ì¥ì†Œë“¤ ì¸ë±ìŠ¤
    no_coord_indices = [i for i in range(len(places)) if i not in visited]

    # ìµœì í™”ëœ ìˆœì„œë¡œ ì¬ë°°ì—´
    optimized = [places[i] for i in route] + [places[i] for i in no_coord_indices]

    print(f"[ROUTE] Optimized route order: {[places[i].get('title', '')[:10] for i in route[:5]]}...")

    return optimized


# ========== MCP Server ì„¤ì • ==========
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://localhost:8000")

# ========== ëª¨ë¸ ë¡œë”© ==========
print("Loading model...")
model_id = "LGAI-EXAONE/EXAONE-3.5-32B-Instruct"

print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_memory={0: "100GiB", "cpu": "50GiB"},
    offload_buffers=True,
)

device = next(model.parameters()).device
print(f"Model loaded! Device: {device}")

# ========== MCP ë„êµ¬ ì •ì˜ (tour-mcp-server ê¸°ì¤€) ==========
MCP_TOOLS = [
    {
        "name": "get_area_codes",
        "description": "ì§€ì—­ì½”ë“œ/ì‹œêµ°êµ¬ì½”ë“œ ëª©ë¡ ì¡°íšŒ",
        "parameters": {"area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ"}
    },
    {
        "name": "search_by_area",
        "description": "ì§€ì—­ê¸°ë°˜ ê´€ê´‘ì •ë³´ ê²€ìƒ‰. íŠ¹ì • ì§€ì—­ì˜ ê´€ê´‘ì§€, ìŒì‹ì , ìˆ™ë°• ë“± ì¡°íšŒ",
        "parameters": {
            "area_code": "í•„ìˆ˜ - ì§€ì—­ì½”ë“œ",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ",
            "num_of_rows": "ì„ íƒ - ê²°ê³¼ê°œìˆ˜"
        }
    },
    {
        "name": "search_by_keyword",
        "description": "í‚¤ì›Œë“œë¡œ ê´€ê´‘ì •ë³´ ê²€ìƒ‰. ê°€ì¥ ìœ ì—°í•œ ê²€ìƒ‰ ë°©ë²•",
        "parameters": {
            "keyword": "í•„ìˆ˜ - ê²€ìƒ‰í‚¤ì›Œë“œ",
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "num_of_rows": "ì„ íƒ - ê²°ê³¼ê°œìˆ˜"
        }
    },
    {
        "name": "search_by_location",
        "description": "GPS ìœ„ì¹˜ ê¸°ë°˜ ì£¼ë³€ ê´€ê´‘ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "map_x": "í•„ìˆ˜ - ê²½ë„",
            "map_y": "í•„ìˆ˜ - ìœ„ë„",
            "radius": "ì„ íƒ - ë°˜ê²½(ë¯¸í„°)",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "search_festivals",
        "description": "ì¶•ì œ/í–‰ì‚¬ ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "event_start_date": "í•„ìˆ˜ - ì‹œì‘ì¼(YYYYMMDD)",
            "event_end_date": "ì„ íƒ - ì¢…ë£Œì¼(YYYYMMDD)",
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ"
        }
    },
    {
        "name": "search_stays",
        "description": "ìˆ™ë°• ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ"
        }
    },
    {
        "name": "get_detail_common",
        "description": "ê´€ê´‘ì§€ ìƒì„¸ì •ë³´ ì¡°íšŒ (ì£¼ì†Œ, ì´ë¯¸ì§€, ê°œìš” ë“±)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_detail_intro",
        "description": "ê´€ê´‘ì§€ ì†Œê°œì •ë³´ ì¡°íšŒ (ìš´ì˜ì‹œê°„, ì…ì¥ë£Œ ë“±)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_detail_images",
        "description": "ê´€ê´‘ì§€ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ",
        "parameters": {"content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID"}
    },
    {
        "name": "get_category_codes",
        "description": "ì„œë¹„ìŠ¤ ë¶„ë¥˜ì½”ë“œ ì¡°íšŒ",
        "parameters": {
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "cat1": "ì„ íƒ - ëŒ€ë¶„ë¥˜",
            "cat2": "ì„ íƒ - ì¤‘ë¶„ë¥˜"
        }
    },
    {
        "name": "get_detail_info",
        "description": "ì†Œê°œì •ë³´ ì¡°íšŒ (ë°˜ë³µì •ë³´)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_pet_tour_info",
        "description": "ë°˜ë ¤ë™ë¬¼ ì—¬í–‰ì •ë³´ ì¡°íšŒ",
        "parameters": {
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ"
        }
    }
]

# ì§€ì—­ì½”ë“œ ë§¤í•‘
AREA_CODES = {
    "ì„œìš¸": "1", "ì¸ì²œ": "2", "ëŒ€ì „": "3", "ëŒ€êµ¬": "4", "ê´‘ì£¼": "5",
    "ë¶€ì‚°": "6", "ìš¸ì‚°": "7", "ì„¸ì¢…": "8", "ê²½ê¸°": "31", "ê°•ì›": "32",
    "ì¶©ë¶": "33", "ì¶©ë‚¨": "34", "ê²½ë¶": "35", "ê²½ë‚¨": "36",
    "ì „ë¶": "37", "ì „ë‚¨": "38", "ì œì£¼": "39"
}

CONTENT_TYPES = {
    "ê´€ê´‘ì§€": "12", "ë¬¸í™”ì‹œì„¤": "14", "ì¶•ì œ": "15", "ì—¬í–‰ì½”ìŠ¤": "25",
    "ë ˆí¬ì¸ ": "28", "ìˆ™ë°•": "32", "ì‡¼í•‘": "38", "ìŒì‹ì ": "39", "ì¹´í˜": "39"
}

# Need íƒ€ì… â†’ Content Type ë§¤í•‘
NEED_TO_CONTENT_TYPE = {
    "food": "39",      # ìŒì‹ì 
    "cafe": "39",      # ì¹´í˜ (ìŒì‹ì ê³¼ ë™ì¼)
    "spot": "12",      # ê´€ê´‘ì§€
    "stay": "32",      # ìˆ™ë°•
    "culture": "14",   # ë¬¸í™”ì‹œì„¤
    "shopping": "38",  # ì‡¼í•‘
    "festival": "15",  # ì¶•ì œ
}

# ìµœì†Œ ê²°ê³¼ ê°œìˆ˜ ê¸°ì¤€
MIN_RESULTS_THRESHOLD = 3

# ğŸ”´ LLM í‚¤ì›Œë“œ ì •ê·œí™” ìºì‹œ (ë™ì¼ í‚¤ì›Œë“œ ë°˜ë³µ ìš”ì²­ ë°©ì§€)
KEYWORD_CACHE = {}


def normalize_keywords_batch(user_keywords: list[str]) -> dict[str, list[str]]:
    """
    LLMì„ ì‚¬ìš©í•´ ì—¬ëŸ¬ ì‚¬ìš©ì í‚¤ì›Œë“œë¥¼ í•œë²ˆì— API ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ë³€í™˜ (ë°°ì¹˜ ì²˜ë¦¬)

    Input: ["ê³ ê¹ƒì§‘", "íšŸì§‘", "ì¼ì‹"]
    Output: {
        "ê³ ê¹ƒì§‘": ["ê³ ê¸°", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„"],
        "íšŸì§‘": ["íšŸì§‘", "íšŒ", "í•´ë¬¼"],
        "ì¼ì‹": ["ì´ˆë°¥", "ë¼ë©˜", "ìŠ¤ì‹œ"]
    }
    """
    result = {}
    uncached_keywords = []

    # ìºì‹œëœ ê²ƒ ë¨¼ì € ì²˜ë¦¬
    for kw in user_keywords:
        if kw in KEYWORD_CACHE:
            result[kw] = KEYWORD_CACHE[kw]
            print(f"[NORMALIZE] Cache hit: '{kw}' â†’ {KEYWORD_CACHE[kw]}")
        else:
            uncached_keywords.append(kw)

    # ìºì‹œì— ì—†ëŠ” ê²ƒë§Œ LLM í˜¸ì¶œ (ë°°ì¹˜)
    if not uncached_keywords:
        return result

    keywords_str = ", ".join(uncached_keywords)
    prompt = f"""í•œêµ­ê´€ê´‘ê³µì‚¬ APIì—ì„œ ìŒì‹ì ì„ ê²€ìƒ‰í•˜ë ¤ê³  í•©ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ë‹¤ìŒ ìŒì‹ì ë“¤ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤: {keywords_str}

APIëŠ” ìŒì‹ì  ì´ë¦„ì— í¬í•¨ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
ê° í‚¤ì›Œë“œë³„ë¡œ ìŒì‹ì  ì´ë¦„ì— ìì£¼ í¬í•¨ë˜ëŠ” ê²€ìƒ‰ì–´ 3ê°œì”© ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
- ê³ ê¹ƒì§‘ â†’ ["ê³ ê¸°", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„"]
- íšŸì§‘ â†’ ["íšŸì§‘", "íšŒ", "í•´ë¬¼"]
- ì¼ì‹ì§‘ â†’ ["ì´ˆë°¥", "ë¼ë©˜", "ìŠ¤ì‹œ"]
- ì¤‘êµ­ì§‘ â†’ ["ì§¬ë½•", "ì¤‘í™”", "ë°˜ì "]
- ì¹˜í‚¨ì§‘ â†’ ["ì¹˜í‚¨", "í†µë‹­", "í›„ë¼ì´ë“œ"]
- ì¹´í˜ â†’ ["ì¹´í˜", "ì»¤í”¼", "ë² ì´ì»¤ë¦¬"]

ì‘ë‹µ í˜•ì‹ (JSONë§Œ, ì„¤ëª… ì—†ì´):
{{
  "í‚¤ì›Œë“œ1": ["ê²€ìƒ‰ì–´1", "ê²€ìƒ‰ì–´2", "ê²€ìƒ‰ì–´3"],
  "í‚¤ì›Œë“œ2": ["ê²€ìƒ‰ì–´1", "ê²€ìƒ‰ì–´2", "ê²€ìƒ‰ì–´3"]
}}"""

    messages = [{"role": "user", "content": prompt}]

    try:
        response = generate_response(messages, max_tokens=300, temperature=0.1)
        print(f"[NORMALIZE] LLM batch response: {response}")

        # JSON íŒŒì‹±
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        if json_start >= 0 and json_end > json_start:
            parsed = json.loads(response[json_start:json_end])
            if isinstance(parsed, dict):
                for kw in uncached_keywords:
                    if kw in parsed and isinstance(parsed[kw], list):
                        result[kw] = parsed[kw]
                        KEYWORD_CACHE[kw] = parsed[kw]  # ìºì‹œ ì €ì¥
                        print(f"[NORMALIZE] '{kw}' â†’ {parsed[kw]}")
                    else:
                        # LLMì´ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì€ ê²½ìš°
                        result[kw] = [kw]
                        print(f"[NORMALIZE] '{kw}' â†’ fallback to original")
                return result
    except Exception as e:
        print(f"[NORMALIZE] Batch error: {e}")

    # ì‹¤íŒ¨ì‹œ ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
    for kw in uncached_keywords:
        result[kw] = [kw]
    return result


def normalize_keyword_with_llm(user_keyword: str) -> list[str]:
    """ë‹¨ì¼ í‚¤ì›Œë“œ ì •ê·œí™” (ë°°ì¹˜ í•¨ìˆ˜ì˜ wrapper)"""
    result = normalize_keywords_batch([user_keyword])
    return result.get(user_keyword, [user_keyword])


# ========== Request/Response ëª¨ë¸ ==========
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str = "LGAI-EXAONE/EXAONE-3.5-32B-Instruct"
    messages: list[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7

class MCPQueryRequest(BaseModel):
    query: str  # ìì—°ì–´ ì¿¼ë¦¬: "ê°•ë¦‰ ë°”ë‹¤ ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
    area_code: Optional[str] = None  # ëª¨ë°”ì¼ì—ì„œ ì„ íƒí•œ ë„ ì½”ë“œ (ì˜ˆ: "32" for ê°•ì›)
    sigungu_code: Optional[str] = None  # ëª¨ë°”ì¼ì—ì„œ ì„ íƒí•œ ì‹œ/êµ°/êµ¬ ì½”ë“œ
    max_tokens: int = 1024
    temperature: float = 0.3


# ========== LLM ìƒì„± í•¨ìˆ˜ ==========
def generate_response(messages: list[dict], max_tokens: int = 512, temperature: float = 0.7) -> str:
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.eos_token_id,
        )

    return tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )


# ========== MCP ë„êµ¬ í˜¸ì¶œ ==========
async def call_mcp_tool(tool_name: str, arguments: dict) -> dict:
    """MCP ì„œë²„ì˜ ë„êµ¬ í˜¸ì¶œ (HTTP API)"""
    async with httpx.AsyncClient(timeout=30) as client:
        try:
            response = await client.post(
                f"{MCP_SERVER_URL}/api/call_tool",
                json={"name": tool_name, "arguments": arguments}
            )
            result = response.json()
            if result.get("success"):
                return result.get("result", {})
            else:
                return {"error": result.get("error", "Unknown error")}
        except Exception as e:
            return {"error": str(e)}


async def call_mcp_tool_direct(tool_name: str, arguments: dict) -> dict:
    """MCP ì„œë²„ ì§ì ‘ í˜¸ì¶œ (HTTP API)"""
    # tour-mcp-serverì˜ í•¨ìˆ˜ë¥¼ ì§ì ‘ HTTPë¡œ í˜¸ì¶œ
    # FastMCPëŠ” ê¸°ë³¸ì ìœ¼ë¡œ stdioì§€ë§Œ, HTTP wrapper ì¶”ê°€ í•„ìš”

    # ì„ì‹œ: ì§ì ‘ Tour API í˜¸ì¶œ
    async with httpx.AsyncClient(timeout=30) as client:
        base_url = "https://apis.data.go.kr/B551011/KorService2"
        api_key = os.getenv("TOUR_API_KEY", "")

        common_params = {
            "serviceKey": api_key,
            "MobileOS": "ETC",
            "MobileApp": "TravelMCP",
            "_type": "json",
            "numOfRows": arguments.get("num_of_rows", 20),
            "arrange": arguments.get("arrange", "B")  # ê¸°ë³¸ê°’: ì¡°íšŒìˆœ(ì¸ê¸°ìˆœ)
        }

        endpoint_map = {
            "get_area_codes": "areaCode2",
            "search_by_area": "areaBasedList2",
            "search_by_keyword": "searchKeyword2",
            "search_by_location": "locationBasedList2",
            "search_festivals": "searchFestival2",
            "search_stays": "searchStay2",
            "get_detail_common": "detailCommon2",
            "get_detail_intro": "detailIntro2",
            "get_detail_images": "detailImage2",
        }

        endpoint = endpoint_map.get(tool_name)
        if not endpoint:
            return {"error": f"Unknown tool: {tool_name}"}

        params = {**common_params}

        # íŒŒë¼ë¯¸í„° ë§¤í•‘
        if "area_code" in arguments:
            params["areaCode"] = arguments["area_code"]
        if "sigungu_code" in arguments:
            params["sigunguCode"] = arguments["sigungu_code"]
        if "content_type_id" in arguments:
            params["contentTypeId"] = arguments["content_type_id"]
        if "keyword" in arguments:
            params["keyword"] = arguments["keyword"]
        if "map_x" in arguments:
            params["mapX"] = arguments["map_x"]
        if "map_y" in arguments:
            params["mapY"] = arguments["map_y"]
        if "radius" in arguments:
            params["radius"] = arguments["radius"]
        if "event_start_date" in arguments:
            params["eventStartDate"] = arguments["event_start_date"]
        if "content_id" in arguments:
            params["contentId"] = arguments["content_id"]
            params["defaultYN"] = "Y"
            params["firstImageYN"] = "Y"
            params["addrinfoYN"] = "Y"
            params["overviewYN"] = "Y"

        try:
            response = await client.get(f"{base_url}/{endpoint}", params=params)
            data = response.json()

            if data["response"]["header"]["resultCode"] != "0000":
                return {"error": data["response"]["header"]["resultMsg"]}

            items = data["response"]["body"].get("items", {})
            if not items:
                return {"items": [], "totalCount": 0}

            item_list = items.get("item", [])
            if isinstance(item_list, dict):
                item_list = [item_list]

            return {
                "items": item_list[:20],  # ìµœëŒ€ 20ê°œ
                "totalCount": data["response"]["body"].get("totalCount", len(item_list))
            }
        except Exception as e:
            return {"error": str(e)}


# ========== ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í—¬í¼ í•¨ìˆ˜ ==========
async def search_by_keyword_direct(keyword: str, area_code: str, sigungu_code: str, content_type_id: str = None, num_rows: int = 20) -> dict:
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ì§ì ‘ í˜¸ì¶œ"""
    args = {
        "keyword": keyword,
        "area_code": area_code,
        "num_of_rows": num_rows
    }
    if sigungu_code:
        args["sigungu_code"] = sigungu_code
    if content_type_id:
        args["content_type_id"] = content_type_id

    return await call_mcp_tool_direct("search_by_keyword", args)


async def search_by_area_direct(area_code: str, sigungu_code: str, content_type_id: str = None, num_rows: int = 20, arrange: str = "B") -> dict:
    """ì§€ì—­ ê¸°ë°˜ ê²€ìƒ‰ ì§ì ‘ í˜¸ì¶œ

    arrange ì˜µì…˜:
    - A: ì œëª©ìˆœ (ê¸°ë³¸)
    - B: ì¡°íšŒìˆœ (ì¸ê¸°ìˆœ) â­ ì¶”ì²œ
    - C: ìˆ˜ì •ì¼ìˆœ
    - D: ìƒì„±ì¼ìˆœ
    """
    args = {
        "area_code": area_code,
        "num_of_rows": num_rows,
        "arrange": arrange  # ì¸ê¸°ìˆœ ì •ë ¬
    }
    if sigungu_code:
        args["sigungu_code"] = sigungu_code
    if content_type_id:
        args["content_type_id"] = content_type_id

    return await call_mcp_tool_direct("search_by_area", args)


def analyze_query_needs(query: str) -> dict:
    """ì¿¼ë¦¬ì—ì„œ í•„ìš”í•œ ê²ƒë“¤ì„ ë¶„ì„ (LLM ì—†ì´ ê·œì¹™ ê¸°ë°˜)

    Returns:
        dict: {
            "food": ["ë§›ì§‘", "ëˆê¹ŒìŠ¤"],  # ì¼ë°˜ + êµ¬ì²´ì  í‚¤ì›Œë“œ
            "food_specific": ["ëˆê¹ŒìŠ¤"],  # êµ¬ì²´ì ì¸ ìŒì‹ë§Œ (ì§ì ‘ ê²€ìƒ‰ìš©)
            "cafe": ["ì¹´í˜"],
            ...
        }
    """
    query_lower = query.lower()
    needs = {}

    # êµ¬ì²´ì ì¸ ìŒì‹ í‚¤ì›Œë“œ (API í‚¤ì›Œë“œ ê²€ìƒ‰ì— ì§ì ‘ ì‚¬ìš©)
    # ğŸ”´ "ì¼ì‹", "ê³ ê¸°" ë“± ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œë„ ì¶”ê°€ - ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •í™•í•œ ê²€ìƒ‰ ìœ„í•´
    specific_food_keywords = [
        # ë©”ë‰´ ì¢…ë¥˜
        "ëˆê¹ŒìŠ¤", "ëˆê°€ìŠ¤", "ì‚¼ê²¹ì‚´", "ì¹˜í‚¨", "í”¼ì", "íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬",
        "ì´ˆë°¥", "íšŒ", "ë¼ë©˜", "ìš°ë™", "ëƒ‰ë©´", "ë§‰êµ­ìˆ˜", "ì¹¼êµ­ìˆ˜", "ì§œì¥ë©´", "ì§¬ë½•",
        "ë–¡ë³¶ì´", "ìˆœëŒ€", "ê¹€ë°¥", "ë¹„ë¹”ë°¥", "ë¶ˆê³ ê¸°", "ê°ˆë¹„", "ì‚¼ê³„íƒ•", "ì„¤ë íƒ•",
        "ìˆœë‘ë¶€", "ë¶€ëŒ€ì°Œê°œ", "ê°ìíƒ•", "ê³±ì°½", "ì¡±ë°œ", "ë³´ìŒˆ", "ì¹˜ì¦ˆ", "ë²„ê±°", "í–„ë²„ê±°",
        "ì•„ì´ìŠ¤í¬ë¦¼", "ë¹™ìˆ˜", "ì™€í”Œ", "ë§ˆì¹´ë¡±", "ì¼€ì´í¬",
        # ìŒì‹ ì¹´í…Œê³ ë¦¬ (KEYWORD_TO_API_KEYWORDS ë§¤í•‘ í™œìš©)
        "ì¼ì‹", "ì¼ì‹ì§‘", "ì¤‘ì‹", "ì¤‘ì‹ì§‘", "í•œì‹", "ì–‘ì‹",
        "ê³ ê¸°", "ê³ ê¹ƒì§‘", "ê³ ê¸°ì§‘", "íšŸì§‘", "í•´ì‚°ë¬¼", "ë¶„ì‹", "ë””ì €íŠ¸"
    ]
    specific_matches = [kw for kw in specific_food_keywords if kw in query_lower]
    if specific_matches:
        needs["food_specific"] = specific_matches  # ì§ì ‘ ê²€ìƒ‰ìš©

    # ìŒì‹ ê´€ë ¨ ì¼ë°˜ í‚¤ì›Œë“œ (êµ¬ì²´ì  í‚¤ì›Œë“œëŠ” ìœ„ì—ì„œ ì²˜ë¦¬)
    food_keywords = ["ë§›ì§‘", "ìŒì‹", "ë°¥", "ì‹ë‹¹", "ë¨¹", "ì ì‹¬", "ì €ë…", "ì•„ì¹¨"]
    food_matches = [kw for kw in food_keywords if kw in query_lower]
    if food_matches or specific_matches:
        needs["food"] = food_matches + specific_matches

    # ì¹´í˜ ê´€ë ¨ í‚¤ì›Œë“œ
    cafe_keywords = ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸", "ë¹µ", "ë² ì´ì»¤ë¦¬", "ë¸ŒëŸ°ì¹˜", "ì°¨", "ìŒë£Œ"]
    cafe_matches = [kw for kw in cafe_keywords if kw in query_lower]
    if cafe_matches:
        needs["cafe"] = cafe_matches

    # ê´€ê´‘ì§€ ê´€ë ¨ í‚¤ì›Œë“œ (ë°”ë‹·ê°€, í•´ì•ˆ ë“± ì¶”ê°€)
    spot_keywords = ["ê´€ê´‘", "ëª…ì†Œ", "ë³¼ê±°ë¦¬", "êµ¬ê²½", "ë°”ë‹¤", "ë°”ë‹·ê°€", "í•´ì•ˆ", "ì‚°", "ê³µì›", "í•´ë³€",
                     "ì „ë§", "ì•¼ê²½", "ì‚¬ì§„", "ì¸ìŠ¤íƒ€", "ë°ì´íŠ¸", "ë“œë¼ì´ë¸Œ", "ìì—°", "í’ê²½", "ê²½ì¹˜", "ì‚°ì±…"]
    spot_matches = [kw for kw in spot_keywords if kw in query_lower]
    if spot_matches:
        needs["spot"] = spot_matches

    # ìˆ™ë°• ê´€ë ¨ í‚¤ì›Œë“œ
    stay_keywords = ["ìˆ™ì†Œ", "í˜¸í…”", "íœì…˜", "ëª¨í…”", "ìˆ™ë°•", "ì ", "ë¬µ", "ë¦¬ì¡°íŠ¸", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤"]
    stay_matches = [kw for kw in stay_keywords if kw in query_lower]
    if stay_matches:
        needs["stay"] = stay_matches

    # ë¬¸í™”ì‹œì„¤ ê´€ë ¨ í‚¤ì›Œë“œ
    culture_keywords = ["ë°•ë¬¼ê´€", "ë¯¸ìˆ ê´€", "ì „ì‹œ", "ê³µì—°", "ì˜í™”", "ë¬¸í™”", "ì—­ì‚¬"]
    culture_matches = [kw for kw in culture_keywords if kw in query_lower]
    if culture_matches:
        needs["culture"] = culture_matches

    # ì•„ë¬´ê²ƒë„ ë§¤ì¹­ ì•ˆë˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ê´€ê´‘ì§€ + ìŒì‹ì 
    if not needs:
        needs["spot"] = ["ê´€ê´‘"]
        needs["food"] = ["ë§›ì§‘"]

    # ğŸ”´ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìˆœì„œëŒ€ë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ê²€ìƒ‰ ë° íë ˆì´ì…˜ì— í™œìš©)
    # ì˜ˆ: "ì¹´í˜ê°”ë‹¤ê°€ ëˆê¹ŒìŠ¤ë¨¹ê³  ì €ë…ì€ íšŒ" â†’ ["ì¹´í˜", "ëˆê¹ŒìŠ¤", "íšŸì§‘"]
    user_order = []
    order_keywords = [
        # (ì¹´í…Œê³ ë¦¬ëª…, [í‚¤ì›Œë“œë“¤], cat3 ì½”ë“œ ë˜ëŠ” None)
        ("ì¹´í˜", ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸"], "A05020900"),
        ("ê´€ê´‘ì§€", ["ë°”ë‹¤", "ë°”ë‹·ê°€", "í•´ë³€", "ê´€ê´‘", "êµ¬ê²½", "ì‚°ì±…", "ê³µì›"], None),
        ("ì¹˜í‚¨", ["ì¹˜í‚¨", "í†µë‹­", "í›„ë¼ì´ë“œ"], "A05020700"),
        ("íšŸì§‘", ["íšŸì§‘", "íšŒ", "í•´ì‚°ë¬¼", "ìƒì„ ", "í•´ë¬¼"], "A05020100"),  # í•œì‹-í•´ë¬¼
        ("ê³ ê¹ƒì§‘", ["ê³ ê¸°", "ê³ ê¹ƒì§‘", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„", "ì†Œê³ ê¸°", "ë¼ì§€"], "A05020100"),  # í•œì‹
        ("ëˆê¹ŒìŠ¤", ["ëˆê¹ŒìŠ¤", "ëˆê°€ìŠ¤", "ê¹ŒìŠ¤"], "A05020200"),  # ì„œì–‘ì‹
        ("ì¼ì‹", ["ì¼ì‹", "ì´ˆë°¥", "ë¼ë©˜", "ìŠ¤ì‹œ", "ìš°ë™"], "A05020300"),
        ("í•œì‹", ["í•œì‹", "í•œì •ì‹", "ë°±ë°˜", "ë¹„ë¹”ë°¥", "ê¹€ì¹˜"], "A05020100"),
        ("ì¤‘ì‹", ["ì¤‘ì‹", "ì¤‘êµ­ì§‘", "ì§œì¥", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡"], "A05020400"),
        ("ì–‘ì‹", ["ì–‘ì‹", "íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬", "í”¼ì"], "A05020200"),
        ("ë¶„ì‹", ["ë¶„ì‹", "ë–¡ë³¶ì´", "ìˆœëŒ€", "ê¹€ë°¥"], "A05020600"),
    ]

    # ì¿¼ë¦¬ì—ì„œ ê° ì¹´í…Œê³ ë¦¬ì˜ ì²« ë“±ì¥ ìœ„ì¹˜ ì°¾ê¸°
    category_positions = []
    for cat_name, keywords, cat3 in order_keywords:
        for kw in keywords:
            pos = query_lower.find(kw)
            if pos >= 0:
                category_positions.append((pos, cat_name, cat3))
                break

    # ë“±ì¥ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    category_positions.sort(key=lambda x: x[0])
    user_order = [(cat, cat3) for _, cat, cat3 in category_positions]

    if user_order:
        needs["user_order"] = user_order  # [(ì¹´í…Œê³ ë¦¬ëª…, cat3ì½”ë“œ), ...]
        print(f"[ORCH] User requested order: {[cat for cat, _ in user_order]}")

    print(f"[ORCH] Analyzed needs: {needs}")
    return needs


async def orchestrated_search(query: str, area_code: str, sigungu_code: str, needs: dict) -> dict:
    """
    ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ëœ ê²€ìƒ‰ - í´ë°± ì „ëµ í¬í•¨

    ì „ëµ:
    0. ğŸ”´ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ ìˆœì„œëŒ€ë¡œ ê²€ìƒ‰ (ì¹´í˜ â†’ ëˆê¹ŒìŠ¤ â†’ íšŒ)
    1. êµ¬ì²´ì ì¸ ìŒì‹ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ (ëˆê¹ŒìŠ¤, í”¼ì ë“±)
    2. í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë„ (ë§¤ì¹­ëœ í‚¤ì›Œë“œë¡œ)
    3. ê²°ê³¼ ë¶€ì¡±ì‹œ â†’ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
    4. ì—¬ì „íˆ ë¶€ì¡±ì‹œ â†’ ì§€ì—­ ì „ì²´ ê²€ìƒ‰
    """
    all_results = {}
    search_log = []

    # ğŸ”´ Strategy 0: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ ìˆœì„œëŒ€ë¡œ ê²€ìƒ‰!
    # ì˜ˆ: "ì¹´í˜ê°”ë‹¤ê°€ ëˆê¹ŒìŠ¤ë¨¹ê³  ì €ë…ì€ íšŒ" â†’ ì¹´í˜, ëˆê¹ŒìŠ¤, íšŸì§‘ ê°ê° ê²€ìƒ‰
    if "user_order" in needs and needs["user_order"]:
        user_order = needs["user_order"]
        print(f"[ORCH] Strategy 0: Searching for user-requested categories: {[cat for cat, _ in user_order]}")

        for cat_name, cat3 in user_order:
            cat_results = {"items": [], "category": cat_name, "cat3": cat3}

            # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤í•‘
            search_keywords = {
                "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë² ì´ì»¤ë¦¬"],
                "ê´€ê´‘ì§€": ["ê´€ê´‘", "ëª…ì†Œ"],
                "ì¹˜í‚¨": ["ì¹˜í‚¨", "í†µë‹­"],
                "íšŸì§‘": ["íšŸì§‘", "íšŒ", "í•´ë¬¼"],
                "ê³ ê¹ƒì§‘": ["ê³ ê¸°", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„"],
                "ëˆê¹ŒìŠ¤": ["ëˆê¹ŒìŠ¤", "ëˆê°€ìŠ¤", "ì¹´ì¸ "],
                "ì¼ì‹": ["ì´ˆë°¥", "ì¼ì‹", "ë¼ë©˜"],
                "í•œì‹": ["í•œì‹", "í•œì •ì‹"],
                "ì¤‘ì‹": ["ì¤‘êµ­ì§‘", "ì§¬ë½•", "ì§œì¥"],
                "ì–‘ì‹": ["íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬", "ì–‘ì‹"],
                "ë¶„ì‹": ["ë¶„ì‹", "ë–¡ë³¶ì´"],
            }

            keywords = search_keywords.get(cat_name, [cat_name])
            content_type = "12" if cat_name == "ê´€ê´‘ì§€" else "39"  # ê´€ê´‘ì§€ë©´ 12, ì•„ë‹ˆë©´ ìŒì‹ì 

            for kw in keywords[:2]:
                print(f"[ORCH] Strategy 0: Searching '{kw}' for category '{cat_name}'")
                result = await search_by_keyword_direct(kw, area_code, sigungu_code, content_type)
                items = result.get("items", [])
                search_log.append(f"user_order:{cat_name}â†’{kw}â†’{len(items)}ê°œ")

                if items:
                    # cat3 í•„í„°ë§ (ì¹´í˜, ëˆê¹ŒìŠ¤ ë“± ì„¸ë¶€ ë¶„ë¥˜)
                    if cat3:
                        filtered_items = [i for i in items if i.get("cat3", "").startswith(cat3[:7])]  # A050209xx ì‹ìœ¼ë¡œ prefix ë§¤ì¹­
                        if filtered_items:
                            items = filtered_items
                            print(f"[ORCH] Filtered by cat3 {cat3}: {len(items)} items")

                    # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                    existing_ids = {i.get("contentid") for i in cat_results["items"]}
                    for item in items:
                        if item.get("contentid") not in existing_ids:
                            # ğŸ”´ ê²€ìƒ‰ëœ ì•„ì´í…œì— ì›ë˜ ìš”ì²­ ì¹´í…Œê³ ë¦¬ íƒœê¹…
                            item["_user_category"] = cat_name
                            cat_results["items"].append(item)
                            existing_ids.add(item.get("contentid"))

                    if len(cat_results["items"]) >= 5:
                        break

            if cat_results["items"]:
                all_results[f"user_{cat_name}"] = cat_results
                print(f"[ORCH] Found {len(cat_results['items'])} items for user category '{cat_name}'")

    # Strategy 1: êµ¬ì²´ì ì¸ ìŒì‹ í‚¤ì›Œë“œ ìµœìš°ì„  ê²€ìƒ‰ (ëˆê¹ŒìŠ¤, í”¼ì ë“±)
    if "food_specific" in needs:
        specific_results = {"items": []}

        # ğŸ”´ LLM ë°°ì¹˜ ì •ê·œí™”: ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•œë²ˆì— ì²˜ë¦¬!
        # ["ê³ ê¹ƒì§‘", "íšŸì§‘", "ì¼ì‹"] â†’ {"ê³ ê¹ƒì§‘": ["ê³ ê¸°",...], "íšŸì§‘": ["íšŒ",...], ...}
        keyword_mapping = normalize_keywords_batch(needs["food_specific"])
        print(f"[ORCH] Strategy 0: Batch normalized {len(keyword_mapping)} keywords")

        for kw, api_keywords in keyword_mapping.items():
            print(f"[ORCH] Strategy 0: '{kw}' â†’ API keywords: {api_keywords}")

            for api_kw in api_keywords[:3]:  # ìµœëŒ€ 3ê°œ API í‚¤ì›Œë“œ ì‹œë„
                print(f"[ORCH] Strategy 0: Searching '{api_kw}' for user keyword '{kw}'")
                result = await search_by_keyword_direct(api_kw, area_code, sigungu_code, "39")  # ìŒì‹ì 
                items = result.get("items", [])
                search_log.append(f"specific:{kw}â†’{api_kw}â†’{len(items)}ê°œ")

                if items:
                    # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                    existing_ids = {i.get("contentid") for i in specific_results["items"]}
                    for item in items:
                        if item.get("contentid") not in existing_ids:
                            specific_results["items"].append(item)
                            existing_ids.add(item.get("contentid"))

                    # í•´ë‹¹ í‚¤ì›Œë“œì—ì„œ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ëª¨ì´ë©´ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ
                    if len([i for i in specific_results["items"]]) >= 5:
                        break

        if specific_results["items"]:
            all_results["food_specific"] = specific_results
            print(f"[ORCH] Found {len(specific_results['items'])} specific food items!")

    for need_type, keywords in needs.items():
        # food_specificì€ ì´ë¯¸ ì²˜ë¦¬ë¨
        if need_type == "food_specific":
            continue

        # ğŸ”´ food_specificì´ ìˆìœ¼ë©´ food needëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€)
        # ì¹˜í‚¨ ê²€ìƒ‰í–ˆìœ¼ë©´ ì¼ë°˜ ìŒì‹ì  ê²€ìƒ‰ ì•ˆí•¨ â†’ ê³ ê¹ƒì§‘/íšŸì§‘ ì„ì„ ë°©ì§€
        if need_type == "food" and "food_specific" in needs:
            print(f"[ORCH] Skipping 'food' need (food_specific already processed)")
            continue

        content_type = NEED_TO_CONTENT_TYPE.get(need_type)
        results_for_need = {"items": []}

        # Strategy 1: í‚¤ì›Œë“œ ê²€ìƒ‰ (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¡œ)
        if keywords and isinstance(keywords, list):
            for kw in keywords[:2]:  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ ì‹œë„
                print(f"[ORCH] Strategy 1: keyword search '{kw}' for {need_type}")
                result = await search_by_keyword_direct(kw, area_code, sigungu_code, content_type)
                items = result.get("items", [])
                search_log.append(f"keyword:{kw}â†’{len(items)}ê°œ")

                if len(items) >= MIN_RESULTS_THRESHOLD:
                    results_for_need = result
                    break
                elif items:
                    # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ì €ì¥
                    results_for_need["items"].extend(items)

        # Strategy 2: ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰ (ê²°ê³¼ ë¶€ì¡±ì‹œ)
        if len(results_for_need.get("items", [])) < MIN_RESULTS_THRESHOLD:
            print(f"[ORCH] Strategy 2: area search with content_type={content_type}")
            result = await search_by_area_direct(area_code, sigungu_code, content_type)
            items = result.get("items", [])
            search_log.append(f"area+type:{content_type}â†’{len(items)}ê°œ")

            if items:
                # ê¸°ì¡´ ê²°ê³¼ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                existing_ids = {i.get("contentid") for i in results_for_need.get("items", [])}
                for item in items:
                    if item.get("contentid") not in existing_ids:
                        results_for_need["items"].append(item)

        # Strategy 3: ë” ë§ì€ ê²°ê³¼ ìš”ì²­ (ê°™ì€ ì¹´í…Œê³ ë¦¬ ìœ ì§€!)
        # ğŸ”´ content_type ì—†ì´ ê²€ìƒ‰í•˜ë©´ ê³ ê¹ƒì§‘/íšŸì§‘ ë“± ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ê°€ ì„ì„ â†’ ì œê±°
        if len(results_for_need.get("items", [])) < MIN_RESULTS_THRESHOLD and content_type:
            print(f"[ORCH] Strategy 3: expanded area search with content_type={content_type}")
            result = await search_by_area_direct(area_code, sigungu_code, content_type, num_rows=30)
            items = result.get("items", [])
            search_log.append(f"area_expanded:{content_type}â†’{len(items)}ê°œ")

            if items:
                existing_ids = {i.get("contentid") for i in results_for_need.get("items", [])}
                for item in items:
                    if item.get("contentid") not in existing_ids:
                        results_for_need["items"].append(item)

        all_results[need_type] = results_for_need
        print(f"[ORCH] {need_type}: {len(results_for_need.get('items', []))} items collected")

    # ê²°ê³¼ í•©ì¹˜ê¸° (ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ ìš°ì„ !)
    combined_items = []
    seen_ids = set()

    # ğŸ”´ 1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ ê²°ê³¼ ë¨¼ì € ì¶”ê°€ (ìˆœì„œëŒ€ë¡œ!)
    user_order = needs.get("user_order", [])
    for cat_name, _ in user_order:
        key = f"user_{cat_name}"
        if key in all_results:
            for item in all_results[key].get("items", []):
                cid = item.get("contentid")
                if cid and cid not in seen_ids:
                    seen_ids.add(cid)
                    combined_items.append(item)
            print(f"[ORCH] Added {len([i for i in combined_items if i.get('_user_category') == cat_name])} items for user category '{cat_name}'")

    # 2. êµ¬ì²´ì ì¸ ìŒì‹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ëˆê¹ŒìŠ¤ ê²€ìƒ‰í–ˆìœ¼ë©´ ëˆê¹ŒìŠ¤ì§‘)
    if "food_specific" in all_results:
        for item in all_results["food_specific"].get("items", []):
            cid = item.get("contentid")
            if cid and cid not in seen_ids:
                seen_ids.add(cid)
                combined_items.append(item)
        print(f"[ORCH] Added specific food items, total now: {len(combined_items)}")

    # 3. ë‚˜ë¨¸ì§€ ê²°ê³¼ ì¶”ê°€
    for need_type, result in all_results.items():
        if need_type == "food_specific" or need_type.startswith("user_"):
            continue  # ì´ë¯¸ ì²˜ë¦¬ë¨
        for item in result.get("items", []):
            cid = item.get("contentid")
            if cid and cid not in seen_ids:
                seen_ids.add(cid)
                combined_items.append(item)

    # ğŸ”´ user_order ì •ë³´ë„ ë°˜í™˜ (curateì—ì„œ í™œìš©)
    return {
        "items": combined_items,
        "totalCount": len(combined_items),
        "search_log": search_log,
        "needs_analyzed": list(needs.keys()),
        "user_order": user_order
    }


# ========== LLM ê¸°ë°˜ ë„êµ¬ ì„ íƒ ==========
def select_tools_with_llm(query: str, area_code: Optional[str] = None, sigungu_code: Optional[str] = None) -> list[dict]:
    """LLMì„ ì‚¬ìš©í•´ ì¿¼ë¦¬ì— ë§ëŠ” ë„êµ¬ì™€ íŒŒë¼ë¯¸í„° ì„ íƒ"""

    tools_description = "\n".join([
        f"- {t['name']}: {t['description']}\n  íŒŒë¼ë¯¸í„°: {t['parameters']}" for t in MCP_TOOLS
    ])

    # area_code + sigungu_codeê°€ ì œê³µëœ ê²½ìš° í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì£¼ì…
    area_context = ""
    if area_code and sigungu_code:
        area_context = f"""
**ğŸ”´ ë§¤ìš° ì¤‘ìš”: ì‚¬ìš©ìê°€ ì´ë¯¸ ì§€ì—­ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤ ğŸ”´**
- area_code: "{area_code}" (ë„/ê´‘ì—­ì‹œ ì½”ë“œ - ë°˜ë“œì‹œ ì‚¬ìš©)
- sigungu_code: "{sigungu_code}" (ì‹œ/êµ°/êµ¬ ì½”ë“œ - ë°˜ë“œì‹œ ì‚¬ìš©)

**ğŸ”´ area_code + sigungu_codeê°€ ì œê³µë˜ë©´ ë°˜ë“œì‹œ search_by_areaë¥¼ ì‚¬ìš©í•˜ì„¸ìš”! ğŸ”´**
- search_by_areaëŠ” í‚¤ì›Œë“œ ì—†ì´ ì§€ì—­+ì½˜í…ì¸ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤
- argumentsì— area_codeì™€ sigungu_code ë‘˜ ë‹¤ ë°˜ë“œì‹œ í¬í•¨!
- í‚¤ì›Œë“œ ê²€ìƒ‰(search_by_keyword)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
"""

    prompt = f"""ë‹¹ì‹ ì€ ì—¬í–‰ ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ë„êµ¬ ì„ íƒ AIì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ì ì ˆí•œ ë„êµ¬ì™€ íŒŒë¼ë¯¸í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

{area_context}
## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
{tools_description}

## ì§€ì—­ì½”ë“œ (area_code) - ì‚¬ìš©ìê°€ ì œê³µí•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì°¸ê³ :
ì„œìš¸=1, ì¸ì²œ=2, ëŒ€ì „=3, ëŒ€êµ¬=4, ê´‘ì£¼=5, ë¶€ì‚°=6, ìš¸ì‚°=7, ì„¸ì¢…=8
ê²½ê¸°=31, ê°•ì›=32, ì¶©ë¶=33, ì¶©ë‚¨=34, ê²½ë¶=35, ê²½ë‚¨=36, ì „ë¶=37, ì „ë‚¨=38, ì œì£¼=39

## ì½˜í…ì¸ íƒ€ì… (content_type_id):
ê´€ê´‘ì§€=12, ë¬¸í™”ì‹œì„¤=14, ì¶•ì œ=15, ì—¬í–‰ì½”ìŠ¤=25, ë ˆí¬ì¸ =28, ìˆ™ë°•=32, ì‡¼í•‘=38, ìŒì‹ì /ì¹´í˜=39

## ì˜ˆì‹œ (area_code + sigungu_code ì œê³µëœ ê²½ìš°) - search_by_area ì‚¬ìš©!:
ì§ˆë¬¸: "ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
ì œê³µëœ area_code: "6", sigungu_code: "7"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_area", "arguments": {{"area_code": "6", "sigungu_code": "7", "content_type_id": "39", "num_of_rows": 20}}}}]}}

ì§ˆë¬¸: "ì¹´í˜ë‘ ê´€ê´‘ì§€ ì•Œë ¤ì¤˜"
ì œê³µëœ area_code: "32", sigungu_code: "1"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_area", "arguments": {{"area_code": "32", "sigungu_code": "1", "content_type_id": "39", "num_of_rows": 15}}}}, {{"name": "search_by_area", "arguments": {{"area_code": "32", "sigungu_code": "1", "content_type_id": "12", "num_of_rows": 15}}}}]}}

ì§ˆë¬¸: "ë°ì´íŠ¸í•˜ê¸° ì¢‹ì€ ê³³"
ì œê³µëœ area_code: "1", sigungu_code: "24"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_area", "arguments": {{"area_code": "1", "sigungu_code": "24", "content_type_id": "12", "num_of_rows": 15}}}}, {{"name": "search_by_area", "arguments": {{"area_code": "1", "sigungu_code": "24", "content_type_id": "39", "num_of_rows": 15}}}}]}}

## ì˜ˆì‹œ (area_codeê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°) - search_by_keyword ì‚¬ìš©:
ì§ˆë¬¸: "ê°•ë¦‰ ë°”ë‹¤ ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "ë§›ì§‘", "area_code": "32", "content_type_id": "39", "num_of_rows": 20}}}}]}}

ì§ˆë¬¸: "ë¶€ì‚° í•´ìš´ëŒ€ ìˆ™ë°•"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "í•´ìš´ëŒ€", "area_code": "6", "content_type_id": "32", "num_of_rows": 20}}}}]}}

## í•µì‹¬ ê·œì¹™:
1. **area_code + sigungu_codeê°€ ì œê³µë˜ë©´ â†’ search_by_area ì‚¬ìš© (í‚¤ì›Œë“œ ê²€ìƒ‰ ê¸ˆì§€)**
2. **search_by_area ì‚¬ìš©ì‹œ area_codeì™€ sigungu_code ë‘˜ ë‹¤ argumentsì— í•„ìˆ˜ í¬í•¨!**
3. **ì§€ì—­ì½”ë“œê°€ ì—†ìœ¼ë©´ â†’ search_by_keyword ì‚¬ìš© (í‚¤ì›Œë“œëŠ” ê°„ë‹¨í•œ ëª…ì‚¬ 1~2ê°œë§Œ)**
4. ìŒì‹ì /ë§›ì§‘/ì¹´í˜ â†’ content_type_id="39"
5. ìˆ™ë°•/í˜¸í…”/íœì…˜ â†’ content_type_id="32"
6. ê´€ê´‘ì§€/ëª…ì†Œ â†’ content_type_id="12"
7. ì—¬ëŸ¬ ì¢…ë¥˜ ìš”ì²­ì‹œ â†’ ë„êµ¬ë¥¼ ì—¬ëŸ¬ ê°œ ì‚¬ìš©
8. num_of_rowsëŠ” 15~20 ê¶Œì¥

## ì‚¬ìš©ì ì§ˆë¬¸:
{query}

## ì‘ë‹µ (JSONë§Œ ì¶œë ¥, ì„¤ëª… ì—†ì´):
{{"tools": [...]}}"""

    messages = [{"role": "user", "content": prompt}]

    print(f"[MCP DEBUG] Sending prompt to LLM for tool selection...")
    print(f"[MCP DEBUG] area_code={area_code}, sigungu_code={sigungu_code}")

    response = generate_response(messages, max_tokens=500, temperature=0.1)

    # ë””ë²„ê·¸: LLMì˜ raw response ì¶œë ¥
    print(f"[MCP DEBUG] LLM raw response:\n{response}")
    print(f"[MCP DEBUG] Response length: {len(response)}")

    # JSON íŒŒì‹± - ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ (bracket counting)
    try:
        json_start = response.find("{")
        if json_start < 0:
            print(f"[MCP DEBUG] No JSON object found in response!")
            return []

        # Bracket countingìœ¼ë¡œ ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ ì°¾ê¸°
        bracket_count = 0
        json_end = -1
        in_string = False
        escape_next = False

        for i, char in enumerate(response[json_start:], start=json_start):
            if escape_next:
                escape_next = False
                continue
            if char == '\\' and in_string:
                escape_next = True
                continue
            if char == '"' and not escape_next:
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == '{':
                bracket_count += 1
            elif char == '}':
                bracket_count -= 1
                if bracket_count == 0:
                    json_end = i + 1
                    break

        print(f"[MCP DEBUG] JSON range: {json_start} to {json_end}")

        if json_end > json_start:
            json_str = response[json_start:json_end]
            print(f"[MCP DEBUG] Extracted JSON:\n{json_str}")

            # LLMì´ JSONì— ì£¼ì„(//)ì„ í¬í•¨ì‹œí‚¤ëŠ” ê²½ìš°ê°€ ìˆì–´ì„œ ì œê±°
            # ë¬¸ìì—´ ë‚´ë¶€ê°€ ì•„ë‹Œ ì£¼ì„ë§Œ ì œê±° (ë¼ì¸ ëê¹Œì§€)
            json_str_clean = re.sub(r'//[^\n]*', '', json_str)
            # ì‰¼í‘œ ë’¤ì— ë°”ë¡œ }ë‚˜ ]ê°€ ì˜¤ëŠ” ê²½ìš° ìˆ˜ì • (trailing comma)
            json_str_clean = re.sub(r',(\s*[}\]])', r'\1', json_str_clean)
            print(f"[MCP DEBUG] Cleaned JSON:\n{json_str_clean}")

            result = json.loads(json_str_clean)
            tools = result.get("tools", [])
            print(f"[MCP DEBUG] Parsed tools count: {len(tools)}")
            return tools
        else:
            print(f"[MCP DEBUG] Could not find matching closing bracket!")
    except json.JSONDecodeError as e:
        print(f"[MCP DEBUG] JSON parsing error: {e}")
        print(f"[MCP DEBUG] Failed JSON string: {json_str if 'json_str' in locals() else 'N/A'}")

    # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    print(f"[MCP DEBUG] Returning empty tools list due to parsing failure")
    return []


def curate_results_with_llm(query: str, tool_results: list[dict], user_order: list = None) -> dict:
    """LLMì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íë ˆì´ì…˜ - spots(ë¦¬ìŠ¤íŠ¸ë·°) + course(ì½”ìŠ¤ë·°) ë¶„ë¦¬

    Args:
        user_order: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ ìˆœì„œ [(ì¹´í…Œê³ ë¦¬ëª…, cat3ì½”ë“œ), ...]
    """
    user_order = user_order or []

    # ğŸ”´ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•´ì„œ ê· í˜•ìˆê²Œ ì„ íƒ
    # 1. ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ (_user_category íƒœê·¸ í™œìš©)
    # 2. ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ (ê´€ê´‘ì§€, ìŒì‹ì , ì¹´í˜ ë“±)
    items_by_category = {
        "12": [],      # ê´€ê´‘ì§€
        "14": [],      # ë¬¸í™”ì‹œì„¤
        "32": [],      # ìˆ™ë°•
        "39": [],      # ìŒì‹ì  (ì¹´í˜ ì œì™¸)
        "cafe": [],    # ì¹´í˜ (ë³„ë„ ë¶„ë¦¬)
    }

    # ğŸ”´ ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ì¶”ê°€
    items_by_user_category = {}
    for cat_name, _ in user_order:
        items_by_user_category[cat_name] = []

    for result in tool_results:
        if "items" in result and result["items"]:
            for item in result["items"]:
                content_type = item.get("contenttypeid", "39")
                cat3 = item.get("cat3", "")

                # ğŸ”´ ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ìš°ì„  ë¶„ë¥˜
                user_cat = item.get("_user_category")
                if user_cat and user_cat in items_by_user_category:
                    items_by_user_category[user_cat].append(item)
                    continue  # ì‚¬ìš©ì ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ë˜ë©´ ê¸°ë³¸ ë¶„ë¥˜ ìŠ¤í‚µ

                # ğŸ”´ ì¹´í˜(A05020900)ëŠ” ë³„ë„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
                if content_type == "39" and cat3 == "A05020900":
                    items_by_category["cafe"].append(item)
                elif content_type in items_by_category:
                    items_by_category[content_type].append(item)
                else:
                    items_by_category["39"].append(item)  # ê¸°ë³¸ê°’: ìŒì‹ì 

    # ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ í†µê³„ ì¶œë ¥
    for cat_name, items in items_by_user_category.items():
        if items:
            print(f"[CURATE] User category '{cat_name}': {len(items)} items")

    # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
    for cat, items in items_by_category.items():
        if items:
            cat_name = {"12": "ê´€ê´‘ì§€", "14": "ë¬¸í™”ì‹œì„¤", "32": "ìˆ™ë°•", "39": "ìŒì‹ì ", "cafe": "ì¹´í˜"}.get(cat, cat)
            print(f"[CURATE] Category {cat_name}: {len(items)} items")

    # ğŸ”´ ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ ìš°ì„  ì„ íƒ!
    MAX_PER_CATEGORY = 8
    results_summary = []

    # 1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ì—ì„œ ë¨¼ì € ì„ íƒ (ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì†Œ 3ê°œ)
    for cat_name, _ in user_order:
        items = items_by_user_category.get(cat_name, [])
        for item in items[:max(3, MAX_PER_CATEGORY)]:  # ìµœì†Œ 3ê°œ, ìµœëŒ€ 8ê°œ
            cat3 = item.get("cat3", "")
            content_type = item.get("contenttypeid", "39")
            # í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜ - ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ëª… ìš°ì„  ì‚¬ìš©
            category_name = cat_name  # ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©

            results_summary.append({
                "title": item.get("title", ""),
                "addr": item.get("addr1", ""),
                "type": content_type,
                "cat3": cat3,
                "category": category_name,  # ì‚¬ìš©ì ìš”ì²­ ì¹´í…Œê³ ë¦¬ëª…!
                "image": item.get("firstimage", ""),
                "mapx": item.get("mapx", ""),
                "mapy": item.get("mapy", ""),
                "tel": item.get("tel", ""),
                "content_id": item.get("contentid", ""),
                "_user_category": cat_name
            })
        print(f"[CURATE] Selected {min(len(items), max(3, MAX_PER_CATEGORY))} items for user category '{cat_name}'")

    # 2. ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¶”ê°€ ì„ íƒ
    for content_type, items in items_by_category.items():
        for item in items[:MAX_PER_CATEGORY]:
            cat3 = item.get("cat3", "")
            # í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜ (LLMì´ ì •í™•í•˜ê²Œ ì´í•´í•˜ë„ë¡)
            category_name = _get_category_name(content_type, cat3)

            results_summary.append({
                "title": item.get("title", ""),
                "addr": item.get("addr1", ""),
                "type": content_type,
                "cat3": cat3,
                "category": category_name,  # í•œê¸€ ì¹´í…Œê³ ë¦¬ëª… ì¶”ê°€!
                "image": item.get("firstimage", ""),
                "mapx": item.get("mapx", ""),  # ê²½ë„
                "mapy": item.get("mapy", ""),  # ìœ„ë„
                "tel": item.get("tel", ""),
                "content_id": item.get("contentid", "")
            })

    print(f"[CURATE] Total balanced results: {len(results_summary)} places")

    if not results_summary:
        return {
            "spots": [],
            "course": None,
            "message": "ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì¥ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        }

    # ğŸ”´ Step 1: ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ - ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì§„ ì¥ì†Œ í•„í„°ë§ (ë°˜ê²½ 10km)
    results_summary = filter_by_geographic_cluster(results_summary, max_radius_km=10.0)
    print(f"[CURATE] After geographic clustering: {len(results_summary)} places")

    # ğŸ”´ Step 2: ë™ì„  ìµœì í™” - Greedy TSPë¡œ ê°€ê¹Œìš´ ìˆœì„œ ì •ë ¬
    results_summary = optimize_route_order(results_summary)
    print(f"[CURATE] Route optimized")

    # ğŸ”´ Step 3: ê±°ë¦¬ ê³„ì‚°í•˜ì—¬ nearby ì •ë³´ ì¶”ê°€ (LLM ì°¸ê³ ìš©)
    results_summary = calculate_nearby_places(results_summary)
    print(f"[CURATE] Added nearby info to {len(results_summary)} places")

    # ğŸ”´ ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì¹´í…Œê³ ë¦¬ ìˆœì„œ ì¶”ì¶œ
    user_categories = []
    query_lower = query.lower()

    # ìˆœì„œëŒ€ë¡œ ë§¤ì¹­ (ì¿¼ë¦¬ì—ì„œ ë“±ì¥í•˜ëŠ” ìˆœì„œëŒ€ë¡œ)
    category_keywords = [
        ("ì¹´í˜", ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸"]),
        ("ê´€ê´‘ì§€", ["ë°”ë‹¤", "ë°”ë‹·ê°€", "í•´ë³€", "ê´€ê´‘", "êµ¬ê²½", "ì‚°ì±…", "ê³µì›"]),
        ("ì¹˜í‚¨", ["ì¹˜í‚¨", "í†µë‹­"]),
        ("íšŸì§‘", ["íšŸì§‘", "íšŒ", "í•´ì‚°ë¬¼", "ìƒì„ "]),
        ("ê³ ê¹ƒì§‘", ["ê³ ê¸°", "ê³ ê¹ƒì§‘", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„", "ì†Œê³ ê¸°", "ë¼ì§€"]),
        ("ëˆê¹ŒìŠ¤", ["ëˆê¹ŒìŠ¤", "ëˆê°€ìŠ¤", "ê¹ŒìŠ¤"]),
        ("ì¼ì‹", ["ì¼ì‹", "ì´ˆë°¥", "ë¼ë©˜", "ìŠ¤ì‹œ"]),
        ("í•œì‹", ["í•œì‹", "í•œì •ì‹", "ë°±ë°˜"]),
        ("ì¤‘ì‹", ["ì¤‘ì‹", "ì¤‘êµ­", "ì§œì¥", "ì§¬ë½•"]),
        ("ì–‘ì‹", ["ì–‘ì‹", "íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬", "í”¼ì"]),
    ]

    # ì¿¼ë¦¬ì—ì„œ ê° ì¹´í…Œê³ ë¦¬ì˜ ì²« ë“±ì¥ ìœ„ì¹˜ ì°¾ê¸°
    category_positions = []
    for cat_name, keywords in category_keywords:
        for kw in keywords:
            pos = query_lower.find(kw)
            if pos >= 0:
                category_positions.append((pos, cat_name))
                break

    # ë“±ì¥ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    category_positions.sort(key=lambda x: x[0])
    user_categories = [cat for _, cat in category_positions]

    user_order_text = ""
    if user_categories:
        user_order_text = f"""
## ğŸ”´ğŸ”´ğŸ”´ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìˆœì„œ (ì´ ìˆœì„œëŒ€ë¡œ ì½”ìŠ¤ êµ¬ì„± í•„ìˆ˜!):
{' â†’ '.join(user_categories)}

**ìœ„ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€ì¼œì„œ ì½”ìŠ¤ë¥¼ êµ¬ì„±í•˜ì„¸ìš”!**
- ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒ
- category í•„ë“œë¥¼ í™•ì¸í•´ì„œ ì˜¬ë°”ë¥¸ ì¥ì†Œ ì„ íƒ
"""
        print(f"[CURATE] User requested order: {' â†’ '.join(user_categories)}")

    prompt = f"""ë‹¹ì‹ ì€ ì½”ë ˆì¼ ë™í–‰ì—´ì°¨ ì—¬í–‰ íë ˆì´í„°ì…ë‹ˆë‹¤.

## ì„œë¹„ìŠ¤ ì»¨í…ìŠ¤íŠ¸:
- ëŒ€ìƒ: **ì»¤í”Œ ì—¬í–‰ê°** (ì½”ë ˆì¼ ë™í–‰ì—´ì°¨ ì„œë¹„ìŠ¤)
- ëª©ì : ê´€ê´‘/ë°ì´íŠ¸
- ë¶„ìœ„ê¸°: ë¡œë§¨í‹±í•˜ê³  íŠ¹ë³„í•œ ì¶”ì–µ ë§Œë“¤ê¸°
{user_order_text}
## ì‚¬ìš©ì ìš”ì²­:
{query}

## ê²€ìƒ‰ëœ ì¥ì†Œë“¤ (ì´ {len(results_summary)}ê°œ):
**nearby í•„ë“œëŠ” í•´ë‹¹ ì¥ì†Œì—ì„œ 5km ì´ë‚´ ê°€ê¹Œìš´ ì¥ì†Œë“¤ì…ë‹ˆë‹¤. ë™ì„  êµ¬ì„±ì— í™œìš©í•˜ì„¸ìš”!**
{json.dumps(results_summary, ensure_ascii=False, indent=2)}

## ì‘ë‹µ í˜•ì‹ (JSONë§Œ ì¶œë ¥, ì„¤ëª… ì—†ì´):
{{
  "course": {{
    "title": "ì½”ìŠ¤ ì œëª© (ì˜ˆ: ê°•ë¦‰ ë°”ë‹¤í–¥ ë°ì´íŠ¸ ì½”ìŠ¤)",
    "stops": [
      {{
        "order": 1,
        "name": "ì¥ì†Œëª…",
        "address": "ì£¼ì†Œ",
        "mapx": "ê²½ë„ê°’",
        "mapy": "ìœ„ë„ê°’",
        "content_id": "ì½˜í…ì¸ ID",
        "category": "ì¹´í˜/ìŒì‹ì /ê´€ê´‘ì§€/ìˆ™ë°•",
        "time": "ì˜¤ì „ 10ì‹œ",
        "duration": "1ì‹œê°„",
        "travel_time_to_next": "ë‹¤ìŒ ì¥ì†Œê¹Œì§€ ì•½ 10ë¶„",
        "reason": "ì»¤í”Œì—ê²Œ ì¶”ì²œí•˜ëŠ” ì´ìœ ",
        "tip": "ë°©ë¬¸ íŒ"
      }}
    ],
    "total_duration": "ì•½ 6ì‹œê°„",
    "summary": "ì½”ìŠ¤ ìš”ì•½ (2-3ë¬¸ì¥, ì»¤í”Œ ì—¬í–‰ ê´€ì )"
  }}
}}

## ì´ë™ì‹œê°„ ê³„ì‚° ê·œì¹™ (travel_time_to_next):
- nearby í•„ë“œì˜ ê±°ë¦¬ ì •ë³´ë¥¼ í™œìš©í•˜ì„¸ìš”
- ê±°ë¦¬ ê¸°ì¤€ ì˜ˆìƒ ì´ë™ì‹œê°„: **1kmë‹¹ ì•½ 3ë¶„** (ì°¨ëŸ‰ ê¸°ì¤€)
  - 1km â†’ ì•½ 3ë¶„
  - 2km â†’ ì•½ 6ë¶„
  - 5km â†’ ì•½ 15ë¶„
  - 10km â†’ ì•½ 30ë¶„
- ë§ˆì§€ë§‰ ì •ì°¨ì§€ëŠ” travel_time_to_next ìƒëµ (null)

## ê·œì¹™:
- **ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìˆœì„œëŒ€ë¡œ** ì½”ìŠ¤ë¥¼ êµ¬ì„±í•˜ì„¸ìš”!
  - ì˜ˆ: "ì¹´í˜ â†’ ì ì‹¬ ì¼ì‹ â†’ ì €ë… ê³ ê¸°" ìš”ì²­ ì‹œ â†’ ì¹´í˜ ë¨¼ì €, ì¼ì‹ì§‘, ê³ ê¸°ì§‘ ìˆœì„œë¡œ!
- 3~6ê°œ ì¥ì†Œë¥¼ **ì‚¬ìš©ì ìš”ì²­ ìˆœì„œ + ë™ì„  ê³ ë ¤**í•˜ì—¬ ì„ ì •
- **nearby í•„ë“œë¥¼ í™œìš©**í•´ì„œ ê°€ê¹Œìš´ ì¥ì†Œë¼ë¦¬ ë¬¶ì–´ì„œ ë™ì„  ìµœì í™”!
  - ì˜ˆ: Aì¥ì†Œì˜ nearbyì— Bì¥ì†Œê°€ ìˆìœ¼ë©´ Aâ†’B ìˆœì„œê°€ ì´ë™ íš¨ìœ¨ì 
- **ì»¤í”Œ ë°ì´íŠ¸ ê´€ì **ì—ì„œ ì¶”ì²œ ì´ìœ  ì‘ì„±
- mapx, mapy ê°’ì´ ìˆëŠ” ì¥ì†Œ ìš°ì„  ì„ íƒ (ì§€ë„ ì—°ë™ìš©)
- content_id ë°˜ë“œì‹œ í¬í•¨ (ìƒì„¸ì •ë³´ ì¡°íšŒìš©)
- ì¤‘ë³µ/ë¹„ìŠ·í•œ ì¥ì†Œ ì œì™¸
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥

## ğŸ”´ğŸ”´ğŸ”´ ì ˆëŒ€ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜:
1. **ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ì½”ìŠ¤ êµ¬ì„±!** (ì¹´í˜â†’ì¼ì‹â†’ê³ ê¸° ìš”ì²­ì‹œ ìˆœì„œ ì§€í‚¤ê¸°)
2. **ê²€ìƒ‰ëœ ì¥ì†Œ ëª©ë¡ì— ìˆëŠ” ì¥ì†Œë§Œ ì„ íƒí•˜ì„¸ìš”!**
3. **ìƒˆë¡œìš´ ì¥ì†Œë¥¼ ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”!** (ì˜ˆ: "í•´ë³€ ì‚°ì±…", "ì¹´í˜ ë°©ë¬¸" ë“± ì„ì˜ ì¶”ê°€ ê¸ˆì§€)
4. **content_idê°€ ì—†ìœ¼ë©´ ê·¸ ì¥ì†ŒëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**

## ğŸ”´ ë§¤ìš° ì¤‘ìš” - ì •í™•í•œ ì •ë³´ ì‚¬ìš©:
- **category í•„ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”** (ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”!)
  - "í•œì‹" â†’ í•œì‹ ìŒì‹ì 
  - "ì¼ì‹" â†’ ì¼ì‹ ìŒì‹ì  (ì´ˆë°¥, ë¼ë©˜ ë“±)
  - "ì„œì–‘ì‹" â†’ ì„œì–‘ ìŒì‹ì  (ëˆê¹ŒìŠ¤, íŒŒìŠ¤íƒ€, ìŠ¤í…Œì´í¬ ë“±)
  - "ì¹´í˜" â†’ ì¹´í˜/ë””ì €íŠ¸
- ì¥ì†Œ ì´ë¦„ë§Œ ë³´ê³  ìŒì‹ ì¢…ë¥˜ë¥¼ **ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”**
- ì˜ˆ: "í™©íƒœì „íŒŒëŠ”ì§‘"ì€ categoryê°€ "í•œì‹"ì´ë©´ í™©íƒœ ì „ë¬¸ í•œì‹ë‹¹ì…ë‹ˆë‹¤ (ê³ ê¹ƒì§‘ ì•„ë‹˜!)"""

    messages = [{"role": "user", "content": prompt}]
    response = generate_response(messages, max_tokens=1500, temperature=0.5)

    print(f"[CURATE DEBUG] LLM response length: {len(response)}")
    print(f"[CURATE DEBUG] LLM response preview: {response[:500]}...")

    # JSON íŒŒì‹± - bracket counting ë°©ì‹ (select_tools_with_llmê³¼ ë™ì¼)
    curated_course = None
    try:
        json_start = response.find("{")
        if json_start < 0:
            print("[CURATE DEBUG] No JSON object found in response!")
        else:
            # Bracket countingìœ¼ë¡œ ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ ì°¾ê¸°
            bracket_count = 0
            json_end = -1
            in_string = False
            escape_next = False

            for i, char in enumerate(response[json_start:], start=json_start):
                if escape_next:
                    escape_next = False
                    continue
                if char == '\\' and in_string:
                    escape_next = True
                    continue
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                if in_string:
                    continue
                if char == '{':
                    bracket_count += 1
                elif char == '}':
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_end = i + 1
                        break

            print(f"[CURATE DEBUG] JSON range: {json_start} to {json_end}")

            if json_end > json_start:
                json_str = response[json_start:json_end]
                # ì£¼ì„ ì œê±° ë° trailing comma ìˆ˜ì •
                json_str_clean = re.sub(r'//[^\n]*', '', json_str)
                json_str_clean = re.sub(r',(\s*[}\]])', r'\1', json_str_clean)

                parsed = json.loads(json_str_clean)
                curated_course = parsed.get("course")
                print(f"[CURATE DEBUG] Successfully parsed course: {curated_course is not None}")
            else:
                print("[CURATE DEBUG] Could not find matching closing bracket!")
    except json.JSONDecodeError as e:
        print(f"[CURATE DEBUG] JSON parsing error: {e}")
        print(f"[CURATE DEBUG] Failed JSON string: {json_str[:500] if 'json_str' in locals() else 'N/A'}...")

    # spots ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì „ì²´ ê²€ìƒ‰ ê²°ê³¼, ì¢Œí‘œ í¬í•¨)
    spots = []
    for r in results_summary:
        spots.append({
            "name": r["title"],
            "address": r["addr"],
            "category": _get_category_name(r["type"], r.get("cat3")),  # cat3ë¡œ ì¹´í˜/ìŒì‹ì  êµ¬ë¶„
            "image_url": r["image"],
            "mapx": r["mapx"],
            "mapy": r["mapy"],
            "tel": r["tel"],
            "content_id": r["content_id"]
        })

    # ğŸ”´ ì½”ìŠ¤ ê±°ë¦¬ ê²€ì¦ ë° ì‹¤ì œ ê±°ë¦¬ ì¶”ê°€
    if curated_course and "stops" in curated_course:
        curated_course = _add_actual_distances_to_course(curated_course)

    return {
        "spots": spots,  # ë¦¬ìŠ¤íŠ¸ ë·°ìš© (ì „ì²´)
        "course": curated_course,  # ì½”ìŠ¤ ë·°ìš© (LLM íë ˆì´ì…˜)
        "message": f"{len(spots)}ê°œì˜ ì¥ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
    }


def _add_actual_distances_to_course(course: dict) -> dict:
    """
    ì½”ìŠ¤ì˜ ê° ì •ì°¨ì§€ ê°„ ì‹¤ì œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶”ê°€
    LLMì´ ì¶”ì •í•œ travel_time_to_nextì™€ ë³„ê°œë¡œ ì‹¤ì œ ê±°ë¦¬ ì œê³µ
    """
    stops = course.get("stops", [])
    if len(stops) < 2:
        return course

    total_distance = 0.0

    for i in range(len(stops)):
        stop = stops[i]

        if i < len(stops) - 1:
            next_stop = stops[i + 1]
            try:
                lat1 = float(stop.get("mapy", 0))
                lon1 = float(stop.get("mapx", 0))
                lat2 = float(next_stop.get("mapy", 0))
                lon2 = float(next_stop.get("mapx", 0))

                if lat1 != 0 and lon1 != 0 and lat2 != 0 and lon2 != 0:
                    dist = haversine_distance(lat1, lon1, lat2, lon2)
                    stop["distance_to_next_km"] = round(dist, 1)
                    total_distance += dist
                    print(f"[DISTANCE] {stop.get('name', '')} â†’ {next_stop.get('name', '')}: {dist:.1f}km")
                else:
                    stop["distance_to_next_km"] = None
            except:
                stop["distance_to_next_km"] = None
        else:
            # ë§ˆì§€ë§‰ ì •ì°¨ì§€
            stop["distance_to_next_km"] = None

    course["total_distance_km"] = round(total_distance, 1)
    print(f"[DISTANCE] Total course distance: {total_distance:.1f}km")

    return course


def _get_category_name(content_type_id: str, cat3: str = None) -> str:
    """content_type_id + cat3ë¥¼ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜

    cat3 ì½”ë“œ (ìŒì‹ì  ì„¸ë¶€ ë¶„ë¥˜):
    - A05020100: í•œì‹
    - A05020200: ì„œì–‘ì‹ (ëˆê¹ŒìŠ¤, íŒŒìŠ¤íƒ€, ìŠ¤í…Œì´í¬ ë“±)
    - A05020300: ì¼ì‹ (ì´ˆë°¥, ë¼ë©˜ ë“±)
    - A05020400: ì¤‘ì‹
    - A05020500: ì•„ì‹œì•„ìŒì‹
    - A05020600: íŒ¨ë°€ë¦¬ë ˆìŠ¤í† ë‘
    - A05020700: ì´ìƒ‰ìŒì‹ì 
    - A05020800: íŒ¨ìŠ¤íŠ¸í‘¸ë“œ
    - A05020900: ì¹´í˜/ì „í†µì°»ì§‘
    """
    # ìŒì‹ì (39)ì¸ ê²½ìš° cat3ë¡œ ì„¸ë¶€ ë¶„ë¥˜
    if content_type_id == "39" and cat3:
        cat3_map = {
            "A05020100": "í•œì‹",
            "A05020200": "ì„œì–‘ì‹",
            "A05020300": "ì¼ì‹",
            "A05020400": "ì¤‘ì‹",
            "A05020500": "ì•„ì‹œì•„ìŒì‹",
            "A05020600": "íŒ¨ë°€ë¦¬ë ˆìŠ¤í† ë‘",
            "A05020700": "ì´ìƒ‰ìŒì‹ì ",
            "A05020800": "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ",
            "A05020900": "ì¹´í˜",
        }
        if cat3 in cat3_map:
            return cat3_map[cat3]

    type_map = {
        "12": "ê´€ê´‘ì§€",
        "14": "ë¬¸í™”ì‹œì„¤",
        "15": "ì¶•ì œ/í–‰ì‚¬",
        "25": "ì—¬í–‰ì½”ìŠ¤",
        "28": "ë ˆí¬ì¸ ",
        "32": "ìˆ™ë°•",
        "38": "ì‡¼í•‘",
        "39": "ìŒì‹ì "
    }
    return type_map.get(content_type_id, "ê¸°íƒ€")


# ========== API ì—”ë“œí¬ì¸íŠ¸ ==========
@app.get("/health")
async def health():
    return {"status": "ok", "mcp_enabled": True}

@app.get("/v1/models")
async def models():
    return {
        "object": "list",
        "data": [{"id": "LGAI-EXAONE/EXAONE-3.5-32B-Instruct", "object": "model"}]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """ê¸°ì¡´ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸"""
    messages = [{"role": m.role, "content": m.content} for m in request.messages]
    response_text = generate_response(messages, request.max_tokens, request.temperature)

    return {
        "id": "chatcmpl-1",
        "object": "chat.completion",
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": response_text},
            "finish_reason": "stop"
        }]
    }


@app.post("/v1/mcp/query")
async def mcp_query(request: MCPQueryRequest):
    """
    MCP Host ì—”ë“œí¬ì¸íŠ¸ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë²„ì „)
    ìì—°ì–´ ì¿¼ë¦¬ â†’ ì¿¼ë¦¬ ë¶„ì„ â†’ í´ë°± ì „ëµ ê²€ìƒ‰ â†’ ê²°ê³¼ íë ˆì´ì…˜

    ì‘ë‹µ êµ¬ì¡°:
    - spots: ë¦¬ìŠ¤íŠ¸ ë·°ìš© (ì „ì²´ ê²€ìƒ‰ ê²°ê³¼, ì¢Œí‘œ í¬í•¨)
    - course: ì½”ìŠ¤ ë·°ìš© (LLMì´ íë ˆì´ì…˜í•œ ë™ì„ )
    """
    request_id = f"mcp_{int(time.time() * 1000)}"
    start_time = time.time()

    query = request.query
    area_code = request.area_code
    sigungu_code = request.sigungu_code

    logger.info("=" * 70)
    logger.info(f"[{request_id}] /v1/mcp/query ìš”ì²­ ì‹œì‘")
    logger.info(f"[{request_id}] ì‹œê°„: {datetime.now().isoformat()}")
    logger.info(f"[{request_id}] ì¿¼ë¦¬: {query}")
    logger.info(f"[{request_id}] area_code: {area_code}, sigungu_code: {sigungu_code}")
    logger.info("=" * 70)

    # area_codeê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ LLM ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©
    if not area_code:
        logger.info(f"[{request_id}] area_code ì—†ìŒ â†’ LLM ê¸°ë°˜ ë„êµ¬ ì„ íƒ ëª¨ë“œ")
        selected_tools = select_tools_with_llm(query, area_code, sigungu_code)

        if not selected_tools:
            elapsed = time.time() - start_time
            logger.warning(f"[{request_id}] ë„êµ¬ ì„ íƒ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            return {
                "success": False,
                "error": "ì ì ˆí•œ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "query": query,
                "spots": [],
                "course": None
            }

        tool_results = []
        for tool in selected_tools:
            logger.debug(f"[{request_id}] MCP ë„êµ¬ í˜¸ì¶œ: {tool.get('name')}")
            result = await call_mcp_tool(tool.get("name"), tool.get("arguments", {}))
            tool_results.append({"result": result})

        curated = curate_results_with_llm(query, [r["result"] for r in tool_results])
        elapsed = time.time() - start_time
        logger.info(f"[{request_id}] LLM ê¸°ë°˜ ëª¨ë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        return {
            "success": True,
            "query": query,
            "spots": curated.get("spots", []),
            "course": curated.get("course"),
            "message": curated.get("message", ""),
            "search_mode": "llm_based"
        }

    # ========== ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“œ ==========
    logger.info(f"[{request_id}] ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“œ ì‹œì‘")

    # Phase 1: ì¿¼ë¦¬ ë¶„ì„ (ê·œì¹™ ê¸°ë°˜ - ë¹ ë¦„)
    phase1_start = time.time()
    needs = analyze_query_needs(query)
    phase1_elapsed = time.time() - phase1_start
    logger.info(f"[{request_id}] [Phase 1] ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {phase1_elapsed:.3f}ì´ˆ)")
    logger.info(f"[{request_id}]   - needs: {list(needs.keys())}")
    if "user_order" in needs:
        logger.info(f"[{request_id}]   - user_order: {[cat for cat, _ in needs['user_order']]}")

    # Phase 2: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê²€ìƒ‰ (í´ë°± ì „ëµ í¬í•¨)
    phase2_start = time.time()
    search_result = await orchestrated_search(query, area_code, sigungu_code, needs)
    phase2_elapsed = time.time() - phase2_start
    logger.info(f"[{request_id}] [Phase 2] ê²€ìƒ‰ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {phase2_elapsed:.2f}ì´ˆ)")
    logger.info(f"[{request_id}]   - ê²€ìƒ‰ ê²°ê³¼: {search_result.get('totalCount')} items")
    logger.info(f"[{request_id}]   - ê²€ìƒ‰ ë¡œê·¸: {search_result.get('search_log')}")

    # Phase 3: ê²°ê³¼ ê²€ì¦ - ìµœì†Œ ê¸°ì¤€ ë¯¸ë‹¬ì‹œ ì¶”ê°€ ê²€ìƒ‰
    if search_result.get("totalCount", 0) < MIN_RESULTS_THRESHOLD:
        logger.warning(f"[{request_id}] [Phase 3] ê²°ê³¼ ë¶€ì¡± ({search_result.get('totalCount')} < {MIN_RESULTS_THRESHOLD}), ê´‘ì—­ ê²€ìƒ‰ ìˆ˜í–‰")
        phase3_start = time.time()
        broad_result = await search_by_area_direct(area_code, sigungu_code, None, num_rows=50)
        if broad_result.get("items"):
            existing_ids = {i.get("contentid") for i in search_result.get("items", [])}
            added_count = 0
            for item in broad_result["items"]:
                if item.get("contentid") not in existing_ids:
                    search_result["items"].append(item)
                    added_count += 1
            search_result["totalCount"] = len(search_result["items"])
            search_result["search_log"].append(f"broad_fallbackâ†’{len(broad_result['items'])}ê°œ")
            logger.info(f"[{request_id}]   - ê´‘ì—­ ê²€ìƒ‰ìœ¼ë¡œ {added_count}ê°œ ì¶”ê°€")
        phase3_elapsed = time.time() - phase3_start
        logger.info(f"[{request_id}] [Phase 3] ê´‘ì—­ ê²€ìƒ‰ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {phase3_elapsed:.2f}ì´ˆ)")

    # Phase 4: LLM íë ˆì´ì…˜ (ì½”ìŠ¤ ìƒì„±) - user_order ì „ë‹¬
    phase4_start = time.time()
    user_order = search_result.get("user_order", [])
    logger.info(f"[{request_id}] [Phase 4] LLM íë ˆì´ì…˜ ì‹œì‘ (ì…ë ¥ items: {len(search_result.get('items', []))}ê°œ)")
    curated = curate_results_with_llm(query, [search_result], user_order=user_order)
    phase4_elapsed = time.time() - phase4_start
    logger.info(f"[{request_id}] [Phase 4] LLM íë ˆì´ì…˜ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {phase4_elapsed:.2f}ì´ˆ)")
    logger.info(f"[{request_id}]   - spots: {len(curated.get('spots', []))}ê°œ")
    logger.info(f"[{request_id}]   - course: {'ìˆìŒ' if curated.get('course') else 'ì—†ìŒ'}")
    if curated.get("course"):
        course = curated["course"]
        logger.info(f"[{request_id}]   - course.title: {course.get('title')}")
        logger.info(f"[{request_id}]   - course.stops: {len(course.get('stops', []))}ê°œ")
        logger.info(f"[{request_id}]   - course.total_distance_km: {course.get('total_distance_km')}")

    # ìµœì¢… ì‘ë‹µ
    total_elapsed = time.time() - start_time
    logger.info("=" * 70)
    logger.info(f"[{request_id}] /v1/mcp/query ìš”ì²­ ì™„ë£Œ")
    logger.info(f"[{request_id}] ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
    logger.info(f"[{request_id}]   - Phase 1 (ë¶„ì„): {phase1_elapsed:.3f}ì´ˆ")
    logger.info(f"[{request_id}]   - Phase 2 (ê²€ìƒ‰): {phase2_elapsed:.2f}ì´ˆ")
    logger.info(f"[{request_id}]   - Phase 4 (íë ˆì´ì…˜): {phase4_elapsed:.2f}ì´ˆ")
    logger.info("=" * 70)

    return {
        "success": True,
        "query": query,
        "area_code": area_code,
        "sigungu_code": sigungu_code,
        "spots": curated.get("spots", []),
        "course": curated.get("course"),
        "message": curated.get("message", ""),
        "search_mode": "orchestrated",
        "search_log": search_result.get("search_log", []),
        "needs_analyzed": search_result.get("needs_analyzed", [])
    }


@app.get("/v1/mcp/tools")
async def list_mcp_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ ëª©ë¡"""
    return {"tools": MCP_TOOLS}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=30000)
