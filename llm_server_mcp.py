"""
EXAONE-3.5-32B Server with MCP Host functionality
LLMì´ MCP ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  í˜¸ì¶œí•˜ëŠ” ê¸°ëŠ¥ í¬í•¨
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
import torch
import httpx
import json
import asyncio
import os
from typing import Optional

app = FastAPI(title="EXAONE-3.5-32B Server + MCP Host")

# ========== MCP Server ì„¤ì • ==========
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://localhost:8000")

# ========== ëª¨ë¸ ë¡œë”© ==========
print("Loading model...")
model_id = "LGAI-EXAONE/EXAONE-3.5-32B-Instruct"

print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_memory={0: "100GiB", "cpu": "50GiB"},
    offload_buffers=True,
)

device = next(model.parameters()).device
print(f"Model loaded! Device: {device}")

# ========== MCP ë„êµ¬ ì •ì˜ (tour-mcp-server ê¸°ì¤€) ==========
MCP_TOOLS = [
    {
        "name": "get_area_codes",
        "description": "ì§€ì—­ì½”ë“œ/ì‹œêµ°êµ¬ì½”ë“œ ëª©ë¡ ì¡°íšŒ",
        "parameters": {"area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ"}
    },
    {
        "name": "search_by_area",
        "description": "ì§€ì—­ê¸°ë°˜ ê´€ê´‘ì •ë³´ ê²€ìƒ‰. íŠ¹ì • ì§€ì—­ì˜ ê´€ê´‘ì§€, ìŒì‹ì , ìˆ™ë°• ë“± ì¡°íšŒ",
        "parameters": {
            "area_code": "í•„ìˆ˜ - ì§€ì—­ì½”ë“œ",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ",
            "num_of_rows": "ì„ íƒ - ê²°ê³¼ê°œìˆ˜"
        }
    },
    {
        "name": "search_by_keyword",
        "description": "í‚¤ì›Œë“œë¡œ ê´€ê´‘ì •ë³´ ê²€ìƒ‰. ê°€ì¥ ìœ ì—°í•œ ê²€ìƒ‰ ë°©ë²•",
        "parameters": {
            "keyword": "í•„ìˆ˜ - ê²€ìƒ‰í‚¤ì›Œë“œ",
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "num_of_rows": "ì„ íƒ - ê²°ê³¼ê°œìˆ˜"
        }
    },
    {
        "name": "search_by_location",
        "description": "GPS ìœ„ì¹˜ ê¸°ë°˜ ì£¼ë³€ ê´€ê´‘ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "map_x": "í•„ìˆ˜ - ê²½ë„",
            "map_y": "í•„ìˆ˜ - ìœ„ë„",
            "radius": "ì„ íƒ - ë°˜ê²½(ë¯¸í„°)",
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "search_festivals",
        "description": "ì¶•ì œ/í–‰ì‚¬ ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "event_start_date": "í•„ìˆ˜ - ì‹œì‘ì¼(YYYYMMDD)",
            "event_end_date": "ì„ íƒ - ì¢…ë£Œì¼(YYYYMMDD)",
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ"
        }
    },
    {
        "name": "search_stays",
        "description": "ìˆ™ë°• ì •ë³´ ê²€ìƒ‰",
        "parameters": {
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ"
        }
    },
    {
        "name": "get_detail_common",
        "description": "ê´€ê´‘ì§€ ìƒì„¸ì •ë³´ ì¡°íšŒ (ì£¼ì†Œ, ì´ë¯¸ì§€, ê°œìš” ë“±)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_detail_intro",
        "description": "ê´€ê´‘ì§€ ì†Œê°œì •ë³´ ì¡°íšŒ (ìš´ì˜ì‹œê°„, ì…ì¥ë£Œ ë“±)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_detail_images",
        "description": "ê´€ê´‘ì§€ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ",
        "parameters": {"content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID"}
    },
    {
        "name": "get_category_codes",
        "description": "ì„œë¹„ìŠ¤ ë¶„ë¥˜ì½”ë“œ ì¡°íšŒ",
        "parameters": {
            "content_type_id": "ì„ íƒ - ì½˜í…ì¸ íƒ€ì…",
            "cat1": "ì„ íƒ - ëŒ€ë¶„ë¥˜",
            "cat2": "ì„ íƒ - ì¤‘ë¶„ë¥˜"
        }
    },
    {
        "name": "get_detail_info",
        "description": "ì†Œê°œì •ë³´ ì¡°íšŒ (ë°˜ë³µì •ë³´)",
        "parameters": {
            "content_id": "í•„ìˆ˜ - ì½˜í…ì¸ ID",
            "content_type_id": "í•„ìˆ˜ - ì½˜í…ì¸ íƒ€ì…"
        }
    },
    {
        "name": "get_pet_tour_info",
        "description": "ë°˜ë ¤ë™ë¬¼ ì—¬í–‰ì •ë³´ ì¡°íšŒ",
        "parameters": {
            "area_code": "ì„ íƒ - ì§€ì—­ì½”ë“œ",
            "sigungu_code": "ì„ íƒ - ì‹œêµ°êµ¬ì½”ë“œ"
        }
    }
]

# ì§€ì—­ì½”ë“œ ë§¤í•‘
AREA_CODES = {
    "ì„œìš¸": "1", "ì¸ì²œ": "2", "ëŒ€ì „": "3", "ëŒ€êµ¬": "4", "ê´‘ì£¼": "5",
    "ë¶€ì‚°": "6", "ìš¸ì‚°": "7", "ì„¸ì¢…": "8", "ê²½ê¸°": "31", "ê°•ì›": "32",
    "ì¶©ë¶": "33", "ì¶©ë‚¨": "34", "ê²½ë¶": "35", "ê²½ë‚¨": "36",
    "ì „ë¶": "37", "ì „ë‚¨": "38", "ì œì£¼": "39"
}

CONTENT_TYPES = {
    "ê´€ê´‘ì§€": "12", "ë¬¸í™”ì‹œì„¤": "14", "ì¶•ì œ": "15", "ì—¬í–‰ì½”ìŠ¤": "25",
    "ë ˆí¬ì¸ ": "28", "ìˆ™ë°•": "32", "ì‡¼í•‘": "38", "ìŒì‹ì ": "39", "ì¹´í˜": "39"
}


# ========== Request/Response ëª¨ë¸ ==========
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str = "LGAI-EXAONE/EXAONE-3.5-32B-Instruct"
    messages: list[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7

class MCPQueryRequest(BaseModel):
    query: str  # ìì—°ì–´ ì¿¼ë¦¬: "ê°•ë¦‰ ë°”ë‹¤ ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
    area_code: Optional[str] = None  # ëª¨ë°”ì¼ì—ì„œ ì„ íƒí•œ ë„ ì½”ë“œ (ì˜ˆ: "32" for ê°•ì›)
    sigungu_code: Optional[str] = None  # ëª¨ë°”ì¼ì—ì„œ ì„ íƒí•œ ì‹œ/êµ°/êµ¬ ì½”ë“œ
    max_tokens: int = 1024
    temperature: float = 0.3


# ========== LLM ìƒì„± í•¨ìˆ˜ ==========
def generate_response(messages: list[dict], max_tokens: int = 512, temperature: float = 0.7) -> str:
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.eos_token_id,
        )

    return tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )


# ========== MCP ë„êµ¬ í˜¸ì¶œ ==========
async def call_mcp_tool(tool_name: str, arguments: dict) -> dict:
    """MCP ì„œë²„ì˜ ë„êµ¬ í˜¸ì¶œ (HTTP API)"""
    async with httpx.AsyncClient(timeout=30) as client:
        try:
            response = await client.post(
                f"{MCP_SERVER_URL}/api/call_tool",
                json={"name": tool_name, "arguments": arguments}
            )
            result = response.json()
            if result.get("success"):
                return result.get("result", {})
            else:
                return {"error": result.get("error", "Unknown error")}
        except Exception as e:
            return {"error": str(e)}


async def call_mcp_tool_direct(tool_name: str, arguments: dict) -> dict:
    """MCP ì„œë²„ ì§ì ‘ í˜¸ì¶œ (HTTP API)"""
    # tour-mcp-serverì˜ í•¨ìˆ˜ë¥¼ ì§ì ‘ HTTPë¡œ í˜¸ì¶œ
    # FastMCPëŠ” ê¸°ë³¸ì ìœ¼ë¡œ stdioì§€ë§Œ, HTTP wrapper ì¶”ê°€ í•„ìš”

    # ì„ì‹œ: ì§ì ‘ Tour API í˜¸ì¶œ
    async with httpx.AsyncClient(timeout=30) as client:
        base_url = "https://apis.data.go.kr/B551011/KorService2"
        api_key = os.getenv("TOUR_API_KEY", "")

        common_params = {
            "serviceKey": api_key,
            "MobileOS": "ETC",
            "MobileApp": "TravelMCP",
            "_type": "json",
            "numOfRows": arguments.get("num_of_rows", 20)
        }

        endpoint_map = {
            "get_area_codes": "areaCode2",
            "search_by_area": "areaBasedList2",
            "search_by_keyword": "searchKeyword2",
            "search_by_location": "locationBasedList2",
            "search_festivals": "searchFestival2",
            "search_stays": "searchStay2",
            "get_detail_common": "detailCommon2",
            "get_detail_intro": "detailIntro2",
            "get_detail_images": "detailImage2",
        }

        endpoint = endpoint_map.get(tool_name)
        if not endpoint:
            return {"error": f"Unknown tool: {tool_name}"}

        params = {**common_params}

        # íŒŒë¼ë¯¸í„° ë§¤í•‘
        if "area_code" in arguments:
            params["areaCode"] = arguments["area_code"]
        if "sigungu_code" in arguments:
            params["sigunguCode"] = arguments["sigungu_code"]
        if "content_type_id" in arguments:
            params["contentTypeId"] = arguments["content_type_id"]
        if "keyword" in arguments:
            params["keyword"] = arguments["keyword"]
        if "map_x" in arguments:
            params["mapX"] = arguments["map_x"]
        if "map_y" in arguments:
            params["mapY"] = arguments["map_y"]
        if "radius" in arguments:
            params["radius"] = arguments["radius"]
        if "event_start_date" in arguments:
            params["eventStartDate"] = arguments["event_start_date"]
        if "content_id" in arguments:
            params["contentId"] = arguments["content_id"]
            params["defaultYN"] = "Y"
            params["firstImageYN"] = "Y"
            params["addrinfoYN"] = "Y"
            params["overviewYN"] = "Y"

        try:
            response = await client.get(f"{base_url}/{endpoint}", params=params)
            data = response.json()

            if data["response"]["header"]["resultCode"] != "0000":
                return {"error": data["response"]["header"]["resultMsg"]}

            items = data["response"]["body"].get("items", {})
            if not items:
                return {"items": [], "totalCount": 0}

            item_list = items.get("item", [])
            if isinstance(item_list, dict):
                item_list = [item_list]

            return {
                "items": item_list[:20],  # ìµœëŒ€ 20ê°œ
                "totalCount": data["response"]["body"].get("totalCount", len(item_list))
            }
        except Exception as e:
            return {"error": str(e)}


# ========== LLM ê¸°ë°˜ ë„êµ¬ ì„ íƒ ==========
def select_tools_with_llm(query: str, area_code: Optional[str] = None, sigungu_code: Optional[str] = None) -> list[dict]:
    """LLMì„ ì‚¬ìš©í•´ ì¿¼ë¦¬ì— ë§ëŠ” ë„êµ¬ì™€ íŒŒë¼ë¯¸í„° ì„ íƒ"""

    tools_description = "\n".join([
        f"- {t['name']}: {t['description']}\n  íŒŒë¼ë¯¸í„°: {t['parameters']}" for t in MCP_TOOLS
    ])

    # area_codeê°€ ì œê³µëœ ê²½ìš° í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì£¼ì…
    area_context = ""
    if area_code:
        area_context = f"""
**ğŸ”´ ì¤‘ìš”: ì‚¬ìš©ìê°€ ì´ë¯¸ ì§€ì—­ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤ ğŸ”´**
- area_code: "{area_code}" (ì´ ì½”ë“œë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•˜ì„¸ìš”. ë‹¤ë¥¸ ì§€ì—­ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”)
"""
        if sigungu_code:
            area_context += f'- sigungu_code: "{sigungu_code}" (ì´ ì½”ë“œë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•˜ì„¸ìš”)\n'
        area_context += "\nì§ˆë¬¸ì—ì„œ ì§€ì—­ëª…ì„ ì¶”ì¶œí•˜ì§€ ë§ê³ , ìœ„ì— ì œê³µëœ area_codeë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n"

    prompt = f"""ë‹¹ì‹ ì€ ì—¬í–‰ ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ë„êµ¬ ì„ íƒ AIì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ì ì ˆí•œ ë„êµ¬ì™€ íŒŒë¼ë¯¸í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

{area_context}
## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
{tools_description}

## ì§€ì—­ì½”ë“œ (area_code) - ì‚¬ìš©ìê°€ ì œê³µí•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì°¸ê³ :
ì„œìš¸=1, ì¸ì²œ=2, ëŒ€ì „=3, ëŒ€êµ¬=4, ê´‘ì£¼=5, ë¶€ì‚°=6, ìš¸ì‚°=7, ì„¸ì¢…=8
ê²½ê¸°=31, ê°•ì›=32, ì¶©ë¶=33, ì¶©ë‚¨=34, ê²½ë¶=35, ê²½ë‚¨=36, ì „ë¶=37, ì „ë‚¨=38, ì œì£¼=39

## ì½˜í…ì¸ íƒ€ì… (content_type_id):
ê´€ê´‘ì§€=12, ë¬¸í™”ì‹œì„¤=14, ì¶•ì œ=15, ì—¬í–‰ì½”ìŠ¤=25, ë ˆí¬ì¸ =28, ìˆ™ë°•=32, ì‡¼í•‘=38, ìŒì‹ì =39

## ì˜ˆì‹œ (area_code + sigungu_code ì œê³µëœ ê²½ìš°):
ì§ˆë¬¸: "ë°”ë‹¤ ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
ì œê³µëœ area_code: "32", sigungu_code: "1"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "ë°”ë‹¤ ë§›ì§‘", "area_code": "32", "sigungu_code": "1", "content_type_id": "39"}}}}]}}

ì§ˆë¬¸: "ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
ì œê³µëœ area_code: "32", sigungu_code: "1"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "ë§›ì§‘", "area_code": "32", "sigungu_code": "1", "content_type_id": "39"}}}}]}}

## ì˜ˆì‹œ (area_codeë§Œ ì œê³µëœ ê²½ìš°):
ì§ˆë¬¸: "ì¡°ìš©í•œ ê´€ê´‘ì§€ ì¶”ì²œ"
ì œê³µëœ area_code: "39"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "ì¡°ìš©í•œ ê´€ê´‘ì§€", "area_code": "39", "content_type_id": "12"}}}}]}}

## ì˜ˆì‹œ (area_codeê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°):
ì§ˆë¬¸: "ê°•ë¦‰ ë°”ë‹¤ ê·¼ì²˜ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "ê°•ë¦‰ ë§›ì§‘", "area_code": "32", "content_type_id": "39", "num_of_rows": 20}}}}]}}

ì§ˆë¬¸: "ë¶€ì‚° í•´ìš´ëŒ€ ê·¼ì²˜ ìˆ™ë°•ê³¼ ë§›ì§‘"
ì‘ë‹µ: {{"tools": [{{"name": "search_by_keyword", "arguments": {{"keyword": "í•´ìš´ëŒ€ ìˆ™ë°•", "area_code": "6", "content_type_id": "32"}}}}, {{"name": "search_by_keyword", "arguments": {{"keyword": "í•´ìš´ëŒ€ ë§›ì§‘", "area_code": "6", "content_type_id": "39"}}}}]}}

## ì¤‘ìš”:
- **area_codeê°€ ìœ„ì— ì œê³µëœ ê²½ìš° ë°˜ë“œì‹œ ê·¸ ê°’ì„ ì‚¬ìš© (ìµœìš°ì„ )**
- **sigungu_codeê°€ ì œê³µëœ ê²½ìš° ë°˜ë“œì‹œ argumentsì— í¬í•¨ (ìµœìš°ì„ )**
- ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ì—ì„œ ì§€ì—­ëª…ì„ ì¶”ì¶œí•˜ì—¬ area_codeë¡œ ë³€í™˜
- ìŒì‹ì /ë§›ì§‘/ì¹´í˜ëŠ” content_type_id="39"
- ìˆ™ë°•/í˜¸í…”/íœì…˜ì€ content_type_id="32"
- ê´€ê´‘ì§€/ëª…ì†ŒëŠ” content_type_id="12"
- optional íŒŒë¼ë¯¸í„°ëŠ” í™•ì‹¤í•œ ê°’ì´ ìˆì„ ë•Œë§Œ í¬í•¨, ì—†ìœ¼ë©´ ìƒëµ
- í‚¤ì›Œë“œ ê²€ìƒ‰(search_by_keyword)ì´ ê°€ì¥ ìœ ì—°í•¨
- ì—¬ëŸ¬ ì¡°ê±´ì´ ìˆìœ¼ë©´ ë„êµ¬ë¥¼ ì—¬ëŸ¬ ê°œ ì‚¬ìš©
- ëª¨ë“  ê°’ì€ ì‹¤ì œ ë°ì´í„°ë§Œ ì…ë ¥ (ì„¤ëª…ë¬¸ ê¸ˆì§€)

## ì‚¬ìš©ì ì§ˆë¬¸:
{query}

## ì‘ë‹µ (JSONë§Œ ì¶œë ¥, ì„¤ëª… ì—†ì´):
{{"tools": [...]}}"""

    messages = [{"role": "user", "content": prompt}]
    response = generate_response(messages, max_tokens=500, temperature=0.1)

    # JSON íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        if json_start >= 0 and json_end > json_start:
            json_str = response[json_start:json_end]
            result = json.loads(json_str)
            return result.get("tools", [])
    except json.JSONDecodeError:
        pass

    # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    return []


def curate_results_with_llm(query: str, tool_results: list[dict]) -> dict:
    """LLMì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íë ˆì´ì…˜"""

    # ê²°ê³¼ ìš”ì•½
    results_summary = []
    for result in tool_results:
        if "items" in result and result["items"]:
            for item in result["items"][:10]:
                results_summary.append({
                    "title": item.get("title", ""),
                    "addr": item.get("addr1", ""),
                    "type": item.get("contenttypeid", ""),
                    "image": item.get("firstimage", "")
                })

    if not results_summary:
        return {
            "course_title": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
            "spots": [],
            "summary": "ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì¥ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        }

    prompt = f"""ë‹¹ì‹ ì€ ì—¬í–‰ ì½”ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ì¥ì†Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì ì˜ ì—¬í–‰ ì½”ìŠ¤ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.

## ì‚¬ìš©ì ìš”ì²­:
{query}

## ê²€ìƒ‰ëœ ì¥ì†Œë“¤ (ì´ {len(results_summary)}ê°œ):
{json.dumps(results_summary, ensure_ascii=False, indent=2)}

## ì‘ë‹µ í˜•ì‹ (JSONë§Œ ì¶œë ¥, ì„¤ëª… ì—†ì´):
{{
  "course_title": "ì½”ìŠ¤ ì œëª©",
  "spots": [
    {{
      "name": "ì¥ì†Œëª…",
      "address": "ì£¼ì†Œ",
      "reason": "ì¶”ì²œ ì´ìœ  (í•œ ë¬¸ì¥)",
      "tip": "ë°©ë¬¸ íŒ (í•œ ë¬¸ì¥)"
    }}
  ],
  "summary": "ì „ì²´ ì½”ìŠ¤ ìš”ì•½ (2-3 ë¬¸ì¥)"
}}

## ê·œì¹™:
- 5~8ê°œ ì¥ì†Œ ì„ ì • (ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ë² ìŠ¤íŠ¸ë§Œ)
- ê°™ì€ ì§€ì—­/í…Œë§ˆë¼ë¦¬ ë¬¶ì–´ì„œ ìˆœì„œ ì •ë¦¬
- ì¤‘ë³µë˜ê±°ë‚˜ ë¹„ìŠ·í•œ ì¥ì†ŒëŠ” ì œì™¸
- ì£¼ì†Œ(addr)ê°€ ìˆëŠ” ì¥ì†Œ ìš°ì„  ì„ íƒ
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥"""

    messages = [{"role": "user", "content": prompt}]
    response = generate_response(messages, max_tokens=1000, temperature=0.5)

    # JSON íŒŒì‹±
    try:
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        if json_start >= 0 and json_end > json_start:
            json_str = response[json_start:json_end]
            return json.loads(json_str)
    except json.JSONDecodeError:
        pass

    # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
    return {
        "course_title": "ì¶”ì²œ ì½”ìŠ¤",
        "spots": [{"name": r["title"], "address": r["addr"], "reason": "ê²€ìƒ‰ ê²°ê³¼"} for r in results_summary[:5]],
        "summary": "ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì…ë‹ˆë‹¤."
    }


# ========== API ì—”ë“œí¬ì¸íŠ¸ ==========
@app.get("/health")
async def health():
    return {"status": "ok", "mcp_enabled": True}

@app.get("/v1/models")
async def models():
    return {
        "object": "list",
        "data": [{"id": "LGAI-EXAONE/EXAONE-3.5-32B-Instruct", "object": "model"}]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """ê¸°ì¡´ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸"""
    messages = [{"role": m.role, "content": m.content} for m in request.messages]
    response_text = generate_response(messages, request.max_tokens, request.temperature)

    return {
        "id": "chatcmpl-1",
        "object": "chat.completion",
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": response_text},
            "finish_reason": "stop"
        }]
    }


@app.post("/v1/mcp/query")
async def mcp_query(request: MCPQueryRequest):
    """
    MCP Host ì—”ë“œí¬ì¸íŠ¸
    ìì—°ì–´ ì¿¼ë¦¬ â†’ LLMì´ ë„êµ¬ ì„ íƒ â†’ ë„êµ¬ ì‹¤í–‰ â†’ ê²°ê³¼ íë ˆì´ì…˜
    """
    query = request.query
    area_code = request.area_code
    sigungu_code = request.sigungu_code

    # 1. LLMìœ¼ë¡œ ë„êµ¬ ì„ íƒ (area ì •ë³´ ì „ë‹¬)
    print(f"[MCP] Query: {query}, area_code: {area_code}, sigungu_code: {sigungu_code}")
    selected_tools = select_tools_with_llm(query, area_code, sigungu_code)
    print(f"[MCP] Selected tools: {selected_tools}")

    if not selected_tools:
        return {
            "success": False,
            "error": "ì ì ˆí•œ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "query": query
        }

    # 2. ì„ íƒëœ ë„êµ¬ë“¤ ì‹¤í–‰
    tool_results = []
    for tool in selected_tools:
        tool_name = tool.get("name")
        arguments = tool.get("arguments", {})

        print(f"[MCP] Calling tool: {tool_name} with {arguments}")
        result = await call_mcp_tool(tool_name, arguments)
        tool_results.append({
            "tool": tool_name,
            "arguments": arguments,
            "result": result
        })
        print(f"[MCP] Tool result: {len(result.get('items', []))} items")

    # 3. ê²°ê³¼ íë ˆì´ì…˜
    curated = curate_results_with_llm(query, [r["result"] for r in tool_results])

    return {
        "success": True,
        "query": query,
        "selected_tools": selected_tools,
        "curated_course": curated,
        "raw_results": tool_results
    }


@app.get("/v1/mcp/tools")
async def list_mcp_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ ëª©ë¡"""
    return {"tools": MCP_TOOLS}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=30000)
