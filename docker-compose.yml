services:
  # EXAONE-3.5-32B + MCP Host (korail 전용)
  # 공유 LLM 서버를 사용하거나, MCP 기능이 필요하면 이 서버 사용
  llm-mcp:
    image: lmsysorg/sglang:latest
    container_name: korail-llm-mcp
    runtime: nvidia
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
      - TOUR_API_KEY=${TOUR_API_KEY}
      - MCP_SERVER_URL=http://host.docker.internal:8000
    ports:
      - "30001:30000"  # 30001로 변경 (공유 LLM은 30000)
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./llm_server_mcp.py:/app/server.py
    working_dir: /app
    command: >
      bash -c "pip install --no-cache-dir accelerate httpx &&
      python server.py"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - korail-network
      - shared-ai-network
    restart: unless-stopped
    profiles:
      - mcp  # docker compose --profile mcp up 으로 실행

  # Qdrant - Vector Database for RAG (korail 전용)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: korail-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    networks:
      - korail-network
      - shared-ai-network
    restart: unless-stopped

networks:
  korail-network:
    name: korail-network
    driver: bridge
  shared-ai-network:
    external: true  # shared-ai-services에서 생성한 네트워크 사용

volumes:
  qdrant_data:
